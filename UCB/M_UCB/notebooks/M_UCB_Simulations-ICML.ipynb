{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median-based Bandits with Unbounded Rewards\n",
    "\n",
    "We consider the class of upper confidence bound bandit algorithms for sequential experiment design problems.\n",
    "We propose a policy, $\\textit{Median-based Upper Confidence Bound}$ (M-UCB), based on the empirical median, that is robust to skewed distributions and outliers. In each round ${t}$ ($t > K$), pick an arm with index\n",
    "\n",
    "$$argmax_{i \\in \\mathcal{K}} \\underbrace{\\hat{m}_{i, T_i(t-1)}}_{\\substack{\\text{Empirical} \\\\ \\text{Median}}} + \\beta  \\underbrace{\\left(\\sqrt{2v_{i,t} \\varepsilon_t} + 2 \\varepsilon_t \\sqrt{\\frac{v_{i,t}}{T_i(t-1)}}\\right)}_{\\text{Confidence Width}},$$\n",
    "\n",
    "\n",
    "where $T_i(t-1)$ is the number of times arm $i$ has been played during first $t-1$ rounds, exploration factor $\\varepsilon_t = \\alpha \\log t$ with $\\alpha$ controlling the exploration rate, hazard factor $v_{i,t} = \\frac{4 }{T_i(t-1) \\hat{L}_{i,T_i(t-1)}^2}$, with $\\hat{L}_{i, T_i(t-1)}$ as the lower bound estimation of hazard rate for reward distribution of arm $i$ at the round $t$.\n",
    "    $\\beta$ is a hyper-parameter balancing the empirical median and confidence width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct to proper path\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from codes.UCB_discrete import M_UCB, UCB1, UCB_V, MV_LCB, Exp3, epsilon_greedy, Median_of_Means_UCB, U_UCB\n",
    "from codes.Environment import Mixture_AbsGau, setup_env, Exp\n",
    "from codes.SimulatedGames import simulate\n",
    "from codes.plots import plot_hist, plot_eva\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations\n",
    "\n",
    "We compare our policy (\\ourpolicy) with four others: 1) U-UCB \\cite{cassel_general_2018} with empirical performance distribution measure as VaR, 2) $\\epsilon$-greedy algorithm, which chooses a random arm with probability $\\epsilon = 0.1$,  and otherwise the arm with highest empirical median, 3) robust UCB-median of mean \\cite{bubeck2013bandits}, which allows heavy-tailed reward distributions. 4) adversarial bandit algorithm Exp3 \\cite{Auer2003adv} which assumes the reward distributions are adversarial.\n",
    "The performance of the algorithms are evaluated in terms of the expected sub-optimal draws. Note the plots are shown in log scale on the vertical axis.\n",
    "\n",
    "Note that the robust UCB-median of means and Exp3 algorithms pick the arm with highest mean as the optimal arm, we thus evaluate these two algorithms with the optimal arm defined as the one with highest mean, as they will be disadvantaged if we choose the optimal arm with the maximum median.\n",
    "Including these two algorithms allows us to benchmark against algorithms that allow\n",
    "heavy-tailed distributions and adversarial problems respectively.\n",
    "All other algorithms use median as summary statistics and evaluated with the optimal arm defined as the one with the highest median.\n",
    "\n",
    "For simulations, we design reward distributions using the special case distributions we listed in Section \\ref{app-sec: special case}. The reward distribution for 3 arms are: A) $|\\mathcal{N}(1.2, 4)|$ B) $|\\mathcal{N}(3,1)|$  C) Exp(3.5). We simulate 50 independent experiments, where each experiment runs 10,000 rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "\n",
    "num_exper = 50\n",
    "num_rounds = 10000\n",
    "num_arms = 3\n",
    "est_flag = True # estimate the lower bound of hazard rate L\n",
    "\n",
    "# environment\n",
    "\n",
    "environments = [\n",
    "    {Mixture_AbsGau: [[3, 1, 0,1, 1], [1.2, 4, 0, 1, 1]], Exp: [1/3.5]}, # mu1, sigma1, mu2, sigma2, p\n",
    "    \n",
    "]\n",
    "\n",
    "rewards_env, medians, means, mvs, samples = setup_env(num_arms, environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arm_name_dict = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C'\n",
    "}\n",
    "\n",
    "for key in medians.keys():\n",
    "    print(key)\n",
    "    medians[key] = list(np.around(np.array(medians[key]),2))\n",
    "    means[key] = list(np.around(np.array(means[key]),2))\n",
    "    mvs[key] = list(np.around(np.array(mvs[key]),2))\n",
    "    t = PrettyTable(['Eva', 'A', 'B', 'C', 'Best Arm'])\n",
    "    t.add_row(['Median'] + medians[key]+ [arm_name_dict[np.argmax(medians[key])]])\n",
    "    t.add_row(['Mean']+ means[key]+ [arm_name_dict[np.argmax(means[key])]])\n",
    "    #t.add_row(['MV']+ mvs[key]+ [arm_name_dict[np.argmin(mvs[key])]])\n",
    "    print(t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments\n",
    "\n",
    "def run_exper(rewards_env, hyperpara_list, num_exper, num_rounds, policy, summary_stats, est_flags = [est_flag]):\n",
    "    for key in rewards_env.keys():\n",
    "        for hyperpara in hyperpara_list:\n",
    "            for est_flag in est_flags:\n",
    "                name = key + '_' + str(num_exper) + '_' + str(num_rounds)\n",
    "                policy_name = str(policy).split('.')[-1].split('\\'')[0] + '-'\n",
    "                subname = policy_name + str(hyperpara)\n",
    "                print(name + subname)\n",
    "                p = IntProgress(max = num_exper)\n",
    "                p.description = 'Running'\n",
    "                display(p)\n",
    "                results[name][subname]= simulate(rewards_env[key],summary_stats[key], policy, num_exper, num_rounds, est_flag, hyperpara, None, p)\n",
    "                \n",
    "def run_exper_diff_para(rewards_env, hyperpara_list, num_exper, num_rounds, policy, summary_stats, est_flags = [est_flag]):\n",
    "    for key in rewards_env.keys():\n",
    "        for est_flag in est_flags:\n",
    "            name = key + '_' + str(num_exper) + '_' + str(num_rounds)\n",
    "            \n",
    "            # setting different parameters to with and without outliers groups\n",
    "            if 'Outlier' in name:\n",
    "                hyperpara = hyperpara_list[-1]\n",
    "            else:\n",
    "                hyperpara = hyperpara_list[0]\n",
    "            policy_name = str(policy).split('.')[-1].split('\\'')[0] + '-'\n",
    "\n",
    "            subname = policy_name + str(hyperpara)\n",
    "            print(name + subname)\n",
    "            p = IntProgress(max = num_exper)\n",
    "            p.description = 'Running'\n",
    "            display(p)\n",
    "            results[name][subname]= simulate(rewards_env[key], summary_stats[key], policy, num_exper, num_rounds, est_flag, hyperpara, None, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = defaultdict(dict)\n",
    "\n",
    "\n",
    "# M-UCB\n",
    "\n",
    "hyperpara_list = [[4,1,1]]\n",
    "run_exper(rewards_env, hyperpara_list, num_exper, num_rounds, M_UCB, medians)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "# Benchmark algorithms\n",
    "\n",
    "# U-UCB\n",
    "\n",
    "hyperpara_list = [[4]]\n",
    "run_exper(rewards_env, hyperpara_list, num_exper, num_rounds, U_UCB, medians)\n",
    "\n",
    "# epsilon_greedy\n",
    "\n",
    "hyperpara_list = [[0.1]]\n",
    "run_exper(rewards_env, hyperpara_list, num_exper, num_rounds, epsilon_greedy, medians)\n",
    "\n",
    "# Median_of_Means_UCB\n",
    "\n",
    "hyperpara_list = [[10, 1, 1]]\n",
    "run_exper(rewards_env, hyperpara_list, num_exper, num_rounds, Median_of_Means_UCB, means)\n",
    "\n",
    "# Exp3\n",
    "\n",
    "hyperpara_list = [[0.5, 0, 30]]\n",
    "run_exper_diff_para(rewards_env, hyperpara_list, num_exper, num_rounds, Exp3, means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_eva(results, 'sd', paper_flag = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance on the simulated distributions.\n",
    "As we expected, our algorithm M-UCB outperforms the heavy-tailed method and the adversarial method. Since the light-tailed and stochastic assumption (e.g. sub-exponential, sub-Gaussian) allows us to design a policy with a faster convergence rate of empirical medians. \n",
    "\n",
    "Compared with algorithms which take median as summary statistics, our algorithm is still the best. As analyzed, U-UCB assumes bounded support of rewards and depends on a slow concentration convergence rate, which makes it have a higher number of sub-optimal draws. The idea of $\\epsilon$-greedy is straight-forward, but it is usually hard to beat empirically. \\citet{tamkind2019} reported similar results, for CVaR regret minimisation, $\\epsilon$-greedy is the best baseline algorithm empirically. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('simulation.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

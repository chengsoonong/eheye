
@article{howard_sequential_2019,
	title = {Sequential estimation of quantiles with applications to {A}/{B}-testing and best-arm identification},
	url = {http://arxiv.org/abs/1906.09712},
	abstract = {Consider the problem of sequentially estimating quantiles of any distribution over a complete, fully-ordered set, based on a stream of i.i.d. observations. We propose new, theoretically sound and practically tight confidence sequences for quantiles, that is, sequences of confidence intervals which are valid uniformly over time. We give two methods for tracking a fixed quantile and two methods for tracking all quantiles simultaneously. Specifically, we provide explicit expressions with small constants for intervals whose widths shrink at the fastest possible \${\textbackslash}sqrt\{t{\textasciicircum}\{-1\} {\textbackslash}log{\textbackslash}log t\}\$ rate, as determined by the law of the iterated logarithm (LIL). As a byproduct, we give a non-asymptotic concentration inequality for the empirical distribution function which holds uniformly over time with the LIL rate, thus strengthening Smirnov's asymptotic empirical process LIL, and extending the famed Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to hold uniformly over all sample sizes while only being about twice as wide in practice. This inequality directly yields sequential analogues of the one- and two-sample Kolmogorov-Smirnov tests, and a test of stochastic dominance. We apply our results to the problem of selecting an arm with an approximately best quantile in a multi-armed bandit framework, proving a state-of-the-art sample complexity bound for a novel allocation strategy. Simulations demonstrate that our method stops with fewer samples than existing methods by a factor of five to fifty. Finally, we show how to compute confidence sequences for the difference between quantiles of two arms in an A/B test, along with corresponding always-valid \$p\$-values.},
	urldate = {2020-01-13},
	journal = {arXiv:1906.09712 [math, stat]},
	author = {Howard, Steven R. and Ramdas, Aaditya},
	month = aug,
	year = {2019},
	note = {arXiv: 1906.09712},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Mathematics - Probability, quantile bai, Statistics - Methodology},
	annote = {Comment: 29 pages, 8 figures},
	file = {1906.09712.pdf:/home/mengyan/Zotero/storage/4BLWD4US/1906.09712.pdf:application/pdf}
}

@article{torossian_x-armed_2019,
	title = {X-{Armed} {Bandits}: {Optimizing} {Quantiles}, {CVaR} and {Other} {Risks}},
	shorttitle = {X-{Armed} {Bandits}},
	url = {http://arxiv.org/abs/1904.08205},
	abstract = {We propose and analyze StoROO, an algorithm for risk optimization on stochastic black-box functions derived from StoOO. Motivated by risk-averse decision making fields like agriculture, medicine, biology or finance, we do not focus on the mean payoff but on generic functionals of the return distribution. We provide a generic regret analysis of StoROO and illustrate its applicability with two examples: the optimization of quantiles and CVaR. Inspired by the bandit literature and black-box mean optimizers, StoROO relies on the possibility to construct confidence intervals for the targeted functional based on random-size samples. We detail their construction in the case of quantiles, providing tight bounds based on Kullback-Leibler divergence. We finally present numerical experiments that show a dramatic impact of tight bounds for the optimization of quantiles and CVaR.},
	urldate = {2020-01-13},
	journal = {arXiv:1904.08205 [cs, stat]},
	author = {Torossian, Léonard and Garivier, Aurélien and Picheny, Victor},
	month = oct,
	year = {2019},
	note = {arXiv: 1904.08205},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, quantile bai},
	file = {1904.08205.pdf:/home/mengyan/Zotero/storage/2NH3WNW6/1904.08205.pdf:application/pdf}
}

@article{szorenyi_qualitative_nodate,
	title = {Qualitative {Multi}-{Armed} {Bandits}: {A} {Quantile}-{Based} {Approach}},
	abstract = {We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative scale that allows for comparison but invalidates arithmetic operations such as averaging. Correspondingly, instead of characterizing an arm in terms of the mean of the underlying distribution, we opt for using a quantile of that distribution as a representative value. We address the problem of quantile-based online learning both for the case of a ﬁnite (pure exploration) and inﬁnite time horizon (cumulative regret minimization). For both cases, we propose suitable algorithms and analyze their properties. These properties are also illustrated by means of ﬁrst experimental studies.},
	language = {en},
	author = {Szorenyi, Balazs and Busa-Fekete, Róbert and Weng, Paul and Hüllermeier, Eyke},
	pages = {16},
	annote = {results for searching: quantile bandits
ICML2015
quantile can be used for qualitative reward
applications mentioned
bounding estimation error for quantiles (Prop 1)
Provide finite(PAC)/ infinite analysis
 
READ on 21/08/2019},
	file = {Szorenyi et al. - Qualitative Multi-Armed Bandits A Quantile-Based .pdf:/home/mengyan/Zotero/storage/ALG6UMX8/Szorenyi et al. - Qualitative Multi-Armed Bandits A Quantile-Based .pdf:application/pdf}
}


@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2020}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

%-------------------------------------------

@article{la2019concentration,
  title={Concentration bounds for cvar estimation: The cases of light-tailed and heavy-tailed distributions},
  author={Prashanth L, A and Jagannathan, Krishna and Kolla, Ravi Kumar},
  journal={arXiv preprint arXiv:1901.00997},
  year={2019}
}

@article{kagrecha2019distribution,
  title={Distribution oblivious, risk-aware algorithms for multi-armed bandits with unbounded rewards},
  author={Kagrecha, Anmol and Nair, Jayakrishnan and Jagannathan, Krishna},
  journal={arXiv preprint arXiv:1906.00569},
  year={2019}
}

@inproceedings{yu2018pure,
  title={Pure Exploration of Multi-Armed Bandits with Heavy-Tailed Payoffs.},
  author={Yu, Xiaotian and Shao, Han and Lyu, Michael R and King, Irwin},
  booktitle={UAI},
  pages={937--946},
  year={2018}
}

@article{bubeck2013bandits,
  title={Bandits with heavy tail},
  author={Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  journal={IEEE Transactions on Information Theory},
  volume={59},
  number={11},
  pages={7711--7717},
  year={2013},
  publisher={IEEE}
}

@book{cox2018analysis,
  title={Analysis of survival data},
  author={Cox, David Roxbee},
  year={2018},
  publisher={Chapman and Hall/CRC}
}

@article{bubeck_regret_2012,
	title = {Regret {Analysis} of {Stochastic} and {Nonstochastic} {Multi}-armed {Bandit} {Problems}},
	volume = {5},
	abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration–exploitation trade-oﬀ. This is the balance between staying with the option that gave highest payoﬀs in the past and exploring new options that might give higher payoﬀs in the future. Although the study of bandit problems dates back to the 1930s, exploration–exploitation trade-oﬀs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is deﬁned by the payoﬀ process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoﬀs and adversarial payoﬀs. Besides the basic setting of ﬁnitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
	language = {en},
	number = {1},
	urldate = {2019-08-28},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Bubeck, Sébastien},
	year = {2012},
	pages = {1--122},
	file = {Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf:/home/admin-u6015325/Zotero/storage/4LEA5ZB8/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf:application/pdf}
}

@article{kolla_concentration_2019,
	title = {Concentration bounds for empirical conditional value-at-risk: {The} unbounded case},
	volume = {47},
	shorttitle = {Concentration bounds for empirical conditional value-at-risk},
	abstract = {Conditional Value-at-Risk (CVaR) is a popular risk measure for modelling losses in the case of a rare but extreme event. We consider the problem of estimating CVaR from i.i.d. samples of an unbounded random variable, which is either sub-Gaussian or sub-exponential. We derive a novel one-sided concentration bound for a natural sample-based CVaR estimator in this setting. Our bound relies on a concentration result for a quantile-based estimator for Value-at-Risk (VaR), which may be of independent interest.},
	number = {1},
	urldate = {2019-09-23},
	journal = {Operations Research Letters},
	author = {Kolla, Ravi Kumar and L.a., Prashanth and P. Bhat, Sanjay and Jagannathan, Krishna},
	month = jan,
	year = {2019},
	keywords = {Concentration bounds, Conditional value-at-risk, Sub-exponential distributions, Sub-Gaussian distributions, Value-at-risk},
	pages = {16--20},
	file = {ScienceDirect Full Text PDF:/home/admin-u6015325/Zotero/storage/73S4NQ52/Kolla et al. - 2019 - Concentration bounds for empirical conditional val.pdf:application/pdf;ScienceDirect Snapshot:/home/admin-u6015325/Zotero/storage/UFDXXEUW/S0167637718303869.html:text/html}
}

@article{rockafellar2000optimization,
  title={Optimization of conditional value-at-risk},
  author={Rockafellar, R Tyrrell and Uryasev, Stanislav and others},
  journal={Journal of risk},
  volume={2},
  pages={21--42},
  year={2000}
}

@article{Auer2003adv,
 author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Freund, Yoav and Schapire, Robert E.},
 title = {The Nonstochastic Multiarmed Bandit Problem},
 journal = {SIAM J. Comput.},
 issue_date = {2003},
 volume = {32},
 number = {1},
 month = jan,
 year = {2003},
 pages = {48--77},
 numpages = {30},
 acmid = {589365},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {adversarial bandit problem, unknown matrix games},
} 


@article{coquelin2007bandit,
  title={Bandit algorithms for tree search},
  author={Coquelin, Pierre-Arnaud and Munos, R{\'e}mi},
  journal={arXiv preprint cs/0703062},
  year={2007}
}

@article{wald1944,
  title={On cumulative sums of random variables},
  author={Wald, Abraham},
  journal={The Annals of Mathematical Statistics},
  year={1944}
}

@article{bernstein1924modification,
  title={On a modification of Chebyshev’s inequality and of the error formula of Laplace},
  author={Bernstein, Sergei},
  journal={Ann. Sci. Inst. Sav. Ukraine, Sect. Math},
  volume={1},
  number={4},
  pages={38--49},
  year={1924}
}

@article{schwartz2017customer,
  title={Customer acquisition via display advertising using multi-armed bandit experiments},
  author={Schwartz, Eric M and Bradlow, Eric T and Fader, Peter S},
  journal={Marketing Science},
  volume={36},
  number={4},
  pages={500--522},
  year={2017},
  publisher={INFORMS}
}

@article{babaioff2015dynamic,
  title={Dynamic pricing with limited supply},
  author={Babaioff, Moshe and Dughmi, Shaddin and Kleinberg, Robert and Slivkins, Aleksandrs},
  journal={ACM Transactions on Economics and Computation (TEAC)},
  volume={3},
  number={1},
  pages={4},
  year={2015},
  publisher={ACM}
}

@inproceedings{li2010contextual,
  title={A contextual-bandit approach to personalized news article recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={661--670},
  year={2010},
  organization={ACM}
}

@article{cassel_general_2018,
	title = {A {General} {Approach} to {Multi}-{Armed} {Bandits} {Under} {Risk} {Criteria}},
	abstract = {Different risk-related criteria have received recent interest in learning problems, where typically each case is treated in a customized manner. In this paper we provide a more systematic approach to analyzing such risk criteria within a stochastic multi-armed bandit (MAB) formulation. We identify a set of general conditions that yield a simple characterization of the oracle rule (which serves as the regret benchmark), and facilitate the design of upper confidence bound (UCB) learning policies. The conditions are derived from problem primitives, primarily focusing on the relation between the arm reward distributions and the (risk criteria) performance metric. Among other things, the work highlights some (possibly non-intuitive) subtleties that differentiate various criteria in conjunction with statistical properties of the arms. Our main findings are illustrated on several widely used objectives such as conditional value-at-risk, mean-variance, Sharpe-ratio, and more.},
	journal = {arXiv:1806.01380 [cs, stat]},
	author = {Cassel, Asaf and Mannor, Shie and Zeevi, Assaf},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted for presentation at Conference on Learning Theory (COLT) 2018},
	annote = {performance criteria are defined by a function Ũ that maps the reward vector to a real-valued number, then maximize the expected performance criterion; regret and proxy regret (proxy regret decomposition); mapping function can be differentiable or non-differentiable (CVaR, VaR); estimation error bounded based on Lipschitz continuity (Definition 3).}
}

@incollection{sani_risk-aversion_2012,
	title = {Risk-{Aversion} in {Multi}-armed {Bandits}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Sani, Amir and Lazaric, Alessandro and Munos, Rémi},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {3275--3283},
}

@inproceedings{maillard_robust_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Robust {Risk}-{Averse} {Stochastic} {Multi}-armed {Bandits}},
	abstract = {We study a variant of the standard stochastic multi-armed bandit problem when one is not interested in the arm with the best mean, but instead in the arm maximizing some coherent risk measure criterion. Further, we are studying the deviations of the regret instead of the less informative expected regret. We provide an algorithm, called RA-UCB to solve this problem, together with a high probability bound on its regret.},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer Berlin Heidelberg},
	author = {Maillard, Odalric-Ambrym},
	editor = {Jain, Sanjay and Munos, Rémi and Stephan, Frank and Zeugmann, Thomas},
	year = {2013},
	keywords = {Multi-armed bandits, coherent risk measure, concentration of measure, cumulant generative function},
	pages = {218--233},
	file = {Springer Full Text PDF:/home/admin-u6015325/Zotero/storage/XV9DAGF9/Maillard - 2013 - Robust Risk-Averse Stochastic Multi-armed Bandits.pdf:application/pdf}
}

@article{panaput_dialysis_2014,
	title = {Dialysis {Dose} and {Risk} {Factors} for {Death} {Among} {ESRD} {Patients} {Treated} with {Twice}-{Weekly} {Hemodialysis}: {A} {Prospective} {Cohort} {Study}},
	volume = {38},
	shorttitle = {Dialysis {Dose} and {Risk} {Factors} for {Death} {Among} {ESRD} {Patients} {Treated} with {Twice}-{Weekly} {Hemodialysis}},
	language = {en},
	number = {3-4},
	journal = {Blood Purification},
	author = {Panaput, Thanachai and Thinkhamrop, Bandit and Domrongkitchaiporn, Somnuek and Sirivongs, Dhavee and Praderm, Laksamon and Anukulanantachai, Jirasak and Kanokkantapong, Chavasak and Tungkasereerak, Pakorn and Pongskul, Cholatip and Anutrakulchai, Sirirat and Keobounma, Thathsalang and Narenpitak, Surapong and Intarawongchot, Pisith and Suwattanasin, Ammrit and Tatiyanupanwong, Sajja and Niwattayakul, Kannika},
	year = {2014},
	pages = {253--262},
	file = {Panaput et al. - 2014 - Dialysis Dose and Risk Factors for Death Among ESR.pdf:/home/admin-u6015325/Zotero/storage/W6AHJNHJ/Panaput et al. - 2014 - Dialysis Dose and Risk Factors for Death Among ESR.pdf:application/pdf}
}

@article{panaphut_ceftriaxone_2003,
	title = {Ceftriaxone {Compared} with {Sodium} {Penicillin} {G} for {Treatment} of {Severe} {Leptospirosis}},
	volume = {36},
	abstract = {Abstract.  A prospective, open-label, randomized trial at Khon Kaen Hospital (Thailand) was conducted from July 2000 through December 2001 to compare the clinic},
	language = {en},
	number = {12},
	journal = {Clinical Infectious Diseases},
	author = {Panaphut, Thanachai and Domrongkitchaiporn, Somnuek and Vibhagool, Asda and Thinkamrop, Bandit and Susaengrat, Wattanachai},
	month = jun,
	year = {2003},
	pages = {1507--1513},
	file = {Full Text PDF:/home/admin-u6015325/Zotero/storage/Y2WW473K/Panaphut et al. - 2003 - Ceftriaxone Compared with Sodium Penicillin G for .pdf:application/pdf;Snapshot:/home/admin-u6015325/Zotero/storage/Q2AQPU4B/297044.html:text/html}
}

@article{rinne_hazard_nodate,
	title = {The {Hazard} {Rate} – {Theory} and {Inference}},
	language = {en},
	author = {Rinne, Horst},
	pages = {296},
	file = {Rinne - The Hazard Rate – Theory and Inference.pdf:/home/admin-u6015325/Zotero/storage/D5QPQF7H/Rinne - The Hazard Rate – Theory and Inference.pdf:application/pdf}
}

@book{wasserman2013all,
  title={All of statistics: a concise course in statistical inference},
  author={Wasserman, Larry},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{Rousseeuw2011,
author = {Rousseeuw, Peter J. and Hubert, Mia},
title = {Robust statistics for outlier detection},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
volume = {1},
number = {1},
pages = {73-79},
abstract = {Abstract When analyzing data, outlying observations cause problems because they may strongly influence the result. Robust statistics aims at detecting the outliers by searching for the model fitted by the majority of the data. We present an overview of several robust methods and outlier detection tools. We discuss robust procedures for univariate, low-dimensional, and high-dimensional data such as estimation of location and scatter, linear regression, principal component analysis, and classification. © 2011 John Wiley \& Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 73-79 DOI: 10.1002/widm.2 This article is categorized under: Algorithmic Development > Biological Data Mining Algorithmic Development > Spatial and Temporal Data Mining Application Areas > Health Care Technologies > Structure Discovery and Clustering},
year = {2011}
}

@article{lecue_robust_2017,
	title = {Robust machine learning by median-of-means : theory and practice},
	shorttitle = {Robust machine learning by median-of-means},
	journal = {arXiv:1711.10306 [math, stat]},
	author = {Lecué, Guillaume and Lerasle, Matthieu},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10306},
	keywords = {Mathematics - Statistics Theory},
	annote = {Comment: 48 pages, 6 figures},
	file = {arXiv\:1711.10306 PDF:/home/admin-u6015325/Zotero/storage/ZPRK87L7/Lecué and Lerasle - 2017 - Robust machine learning by median-of-means  theor.pdf:application/pdf;arXiv.org Snapshot:/home/admin-u6015325/Zotero/storage/G7AMGZTS/1711.html:text/html}
}

@article{huber1964,
author = "Huber, Peter J.",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "03",
number = "1",
pages = "73--101",
publisher = "The Institute of Mathematical Statistics",
title = "Robust Estimation of a Location Parameter",
volume = "35",
year = "1964"
}



@article{muller2000possible,
  title={Possible advantages of a robust evaluation of comparisons},
  author={M{\"u}ller, J{\"o}rg W},
  journal={Journal of research of the National Institute of Standards and Technology},
  volume={105},
  number={4},
  pages={551},
  year={2000},
  publisher={National Institute of Standards and Technology}
}

@article{RYZHOV20111363,
title = "The value of information in multi-armed bandits with exponentially distributed rewards",
journal = "Procedia Computer Science",
volume = "4",
pages = "1363 - 1372",
year = "2011",
note = "Proceedings of the International Conference on Computational Science, ICCS 2011",
author = "Ilya O. Ryzhov and Warren B. Powell",
keywords = "multi-armed bandit, knowledge gradient, optimal learning, exponential rewards",
abstract = "We consider a class of multi-armed bandit problems where the reward obtained by pulling an arm is drawn from an exponential distribution whose parameter is unknown. A Bayesian model with independent gamma priors is used to represent our beliefs and uncertainty about the exponential parameters. We derive a precise expression for the marginal value of information in this problem, which allows us to create a new knowledge gradient (KG) policy for making decisions. The policy is practical and easy to implement, making a case for value of information as a general approach to optimal learning problems with many different types of learning models."
}

@book{foss2011introduction,
  title={An introduction to heavy-tailed and subexponential distributions},
  author={Foss, Sergey and Korshunov, Dmitry and Zachary, Stan and others},
  volume={6},
  year={2011},
  publisher={Springer}
}

@article{lai1985asymptotically,
  title={Asymptotically efficient adaptive allocation rules},
  author={Lai, Tze Leung and Robbins, Herbert},
  journal={Advances in applied mathematics},
  volume={6},
  number={1},
  pages={4--22},
  year={1985},
  publisher={Academic Press}
}

@inproceedings{garivier2011kl,
  title={The KL-UCB algorithm for bounded stochastic bandits and beyond},
  author={Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  booktitle={Proceedings of the 24th annual conference on learning theory},
  pages={359--376},
  year={2011}
}

@article{audibert2009exploration,
  title={Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
  author={Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Theoretical Computer Science},
  volume={410},
  number={19},
  pages={1876--1902},
  year={2009},
  publisher={Elsevier}
}

@article{AlphaGo2016,
title	= {Mastering the game of Go with deep neural networks and tree search},
author	= {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
year	= {2016},
journal	= {Nature},
pages	= {484--503},
volume	= {529}
}



@article{AlphaZero2018,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{thompson1933likelihood,
  title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
  author={Thompson, William R},
  journal={Biometrika},
  volume={25},
  number={3/4},
  pages={285--294},
  year={1933},
  publisher={JSTOR}
}

@article{lattimore2018bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  journal={preprint},
  year={2018}
}

@article{burtini2015survey,
  title={A survey of online experiment design with the stochastic multi-armed bandit},
  author={Burtini, Giuseppe and Loeppky, Jason and Lawrence, Ramon},
  journal={arXiv preprint arXiv:1510.00757},
  year={2015}
}

@Article{Auer2002,
author="Auer, Peter
and Cesa-Bianchi, Nicol{\`o}
and Fischer, Paul",
title="Finite-time Analysis of the Multiarmed Bandit Problem",
journal="Machine Learning",
year="2002",
month="05",
day="01",
volume="47",
number="2",
pages="235--256",
abstract="Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.",
}


@article{boucheron2012,
author = "Boucheron, Stéphane and Thomas, Maud",
fjournal = "Electronic Communications in Probability",
journal = "Electron. Commun. Probab.",
pages = "12 pp.",
pno = "51",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = "Concentration inequalities for order statistics",
volume = "17",
year = "2012"
}

@book{boucheron2013,
  title={Concentration inequalities: A nonasymptotic theory of independence},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year={2013},
  publisher={Oxford university press}
}

%-------------------------------
% Reference for Best Arm Identification literature


@inproceedings{bubeck_pure_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pure {Exploration} in {Multi}-armed {Bandits} {Problems}},
	abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of strategies that perform an online exploration of the arms. The strategies are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. The main result is that the required exploration–exploitation trade-offs are qualitatively different, in view of a general lower bound on the simple regret in terms of the cumulative regret.},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer},
	author = {Bubeck, Sébastien and Munos, Rémi and Stoltz, Gilles},
	editor = {Gavaldà, Ricard and Lugosi, Gábor and Zeugmann, Thomas and Zilles, Sandra},
	year = {2009},
	pages = {23--37},
	file = {Submitted Version:/Users/zhangmengyan/Zotero/storage/B9VDIEN4/Bubeck et al. - 2009 - Pure Exploration in Multi-armed Bandits Problems.pdf:application/pdf}
}

@article{a_concentration_2019,
	title = {Concentration bounds for {CVaR} estimation: {The} cases of light-tailed and heavy-tailed distributions},
	shorttitle = {Concentration bounds for {CVaR} estimation},
	abstract = {Conditional Value-at-Risk (CVaR) is a widely used risk metric in applications such as finance. We derive concentration bounds for CVaR estimates, considering separately the cases of light-tailed and heavy-tailed distributions. In the light-tailed case, we use a classical CVaR estimator based on the empirical distribution constructed from the samples. For heavy-tailed random variables, we assume a mild `bounded moment' condition, and derive a concentration bound for a truncation-based estimator. Notably, our concentration bounds enjoy an exponential decay in the sample size, for heavy-tailed as well as light-tailed distributions. To demonstrate the applicability of our concentration results, we consider a CVaR optimization problem in a multi-armed bandit setting. Specifically, we address the best CVaR-arm identification problem under a fixed budget. We modify the well-known successive rejects algorithm to incorporate a CVaR-based criterion. Using the CVaR concentration result, we derive an upper-bound on the probability of incorrect identification by the proposed algorithm.},
	urldate = {2020-01-11},
	journal = {arXiv:1901.00997 [cs, stat]},
	author = {A., Prashanth L. and Jagannathan, Krishna and Kolla, Ravi Kumar},
	month = aug,
	year = {2019},
	note = {arXiv: 1901.00997},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zhangmengyan/Zotero/storage/KT69PVJP/A. et al. - 2019 - Concentration bounds for CVaR estimation The case.pdf:application/pdf;arXiv.org Snapshot:/Users/zhangmengyan/Zotero/storage/B27PIJDI/1901.html:text/html}
}

@article{kagrecha_distribution_2019,
	title = {Distribution oblivious, risk-aware algorithms for multi-armed bandits with unbounded rewards},
	abstract = {Classical multi-armed bandit problems use the expected value of an arm as a metric to evaluate its goodness. However, the expected value is a risk-neutral metric. In many applications like finance, one is interested in balancing the expected return of an arm (or portfolio) with the risk associated with that return. In this paper, we consider the problem of selecting the arm that optimizes a linear combination of the expected reward and the associated Conditional Value at Risk (CVaR) in a fixed budget best-arm identification framework. We allow the reward distributions to be unbounded or even heavy-tailed. For this problem, our goal is to devise algorithms that are entirely distribution oblivious, i.e., the algorithm is not aware of any information on the reward distributions, including bounds on the moments/tails, or the suboptimality gaps across arms. In this paper, we provide a class of such algorithms with provable upper bounds on the probability of incorrect identification. In the process, we develop a novel estimator for the CVaR of unbounded (including heavy-tailed) random variables and prove a concentration inequality for the same, which could be of independent interest. We also compare the error bounds for our distribution oblivious algorithms with those corresponding to standard non-oblivious algorithms. Finally, numerical experiments reveal that our algorithms perform competitively when compared with non-oblivious algorithms, suggesting that distribution obliviousness can be realised in practice without incurring a significant loss of performance.},
	urldate = {2020-01-11},
	journal = {arXiv:1906.00569 [cs, stat]},
	author = {Kagrecha, Anmol and Nair, Jayakrishnan and Jagannathan, Krishna},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.00569},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zhangmengyan/Zotero/storage/AI237KMM/Kagrecha et al. - 2019 - Distribution oblivious, risk-aware algorithms for .pdf:application/pdf;arXiv.org Snapshot:/Users/zhangmengyan/Zotero/storage/2NSAXBRG/1906.html:text/html}
}

@article{degenne_bridging_2019,
	title = {Bridging the gap between regret minimization and best arm identification, with application to {A}/{B} tests},
	url = {http://arxiv.org/abs/1810.04088},
	abstract = {State of the art online learning procedures focus either on selecting the best alternative ("best arm identification") or on minimizing the cost (the "regret"). We merge these two objectives by providing the theoretical analysis of cost minimizing algorithms that are also delta-PAC (with a proven guaranteed bound on the decision time), hence fulfilling at the same time regret minimization and best arm identification. This analysis sheds light on the common observation that ill-callibrated UCB-algorithms minimize regret while still identifying quickly the best arm. We also extend these results to the non-iid case faced by many practitioners. This provides a technique to make cost versus decision time compromise when doing adaptive tests with applications ranging from website A/B testing to clinical trials.},
	urldate = {2020-01-11},
	journal = {arXiv:1810.04088 [cs, stat]},
	author = {Degenne, Rémy and Nedelec, Thomas and Calauzènes, Clément and Perchet, Vianney},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.04088},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zhangmengyan/Zotero/storage/VYFG6JWD/Degenne et al. - 2019 - Bridging the gap between regret minimization and b.pdf:application/pdf;arXiv.org Snapshot:/Users/zhangmengyan/Zotero/storage/KMRNR28I/1810.html:text/html}
}

@article{jamieson_lil_nodate,
	title = {lil’ {UCB} : {An} {Optimal} {Exploration} {Algorithm} for {Multi}-{Armed} {Bandits}},
	abstract = {The paper proposes a novel upper conﬁdence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the ﬁxed conﬁdence setting using a small number of total samples. The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our conﬁdence bounds to explicitly account for the inﬁnite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCBtype algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art.},
	language = {en},
	author = {Jamieson, Kevin and Malloy, Matthew and Nowak, Robert and Bubeck, Sebastien},
	pages = {17},
	file = {Jamieson et al. - lil’ UCB  An Optimal Exploration Algorithm for Mu.pdf:/Users/zhangmengyan/Zotero/storage/IPF3W9YL/Jamieson et al. - lil’ UCB  An Optimal Exploration Algorithm for Mu.pdf:application/pdf}
}

@incollection{david_pure_2016,
	address = {Cham},
	title = {Pure {Exploration} for {Max}-{Quantile} {Bandits}},
	volume = {9851},
	abstract = {We consider a variant of the pure exploration problem in Multi-Armed Bandits, where the goal is to ﬁnd the arm for which the λ-quantile is maximal. Within the PAC framework, we provide a lower bound on the sample complexity of any ( , δ)-correct algorithm, and propose algorithms with matching upper bounds. Our bounds sharpen existing ones by explicitly incorporating the quantile factor λ. We further provide experiments that compare the sample complexity of our algorithms with that of previous works.},
	language = {en},
	urldate = {2020-01-12},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {David, Yahel and Shimkin, Nahum},
	editor = {Frasconi, Paolo and Landwehr, Niels and Manco, Giuseppe and Vreeken, Jilles},
	year = {2016},
	doi = {10.1007/978-3-319-46128-1_35},
	pages = {556--571},
	file = {David and Shimkin - 2016 - Pure Exploration for Max-Quantile Bandits.pdf:/Users/zhangmengyan/Zotero/storage/ZPFFLBAI/David and Shimkin - 2016 - Pure Exploration for Max-Quantile Bandits.pdf:application/pdf}
}

@inproceedings{david_pac_nodate,
  title={PAC Bandits with Risk Constraints.},
  author={David, Yahel and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Ghavamzadeh, Mohammad and Mannor, Shie and Shimkin, Nahum},
  booktitle={ISAIM},
  year={2018}
}

@incollection{gabillon_best_2012,
	title = {Best {Arm} {Identification}: {A} {Unified} {Approach} to {Fixed} {Budget} and {Fixed} {Confidence}},
	shorttitle = {Best {Arm} {Identification}},
	urldate = {2020-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {3212--3220},
}

@InProceedings{audibert2010best,
author = {Audibert, Jean-Yves and Bubeck, Sébastien},
title = {Best Arm Identification in Multi-Armed Bandits},
booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)},
year = {2010},
month = {January},
abstract = {We consider the problem of finding the best arm in a stochastic multi-armed bandit game. The regret of a forecaster is here defined by the gap between the mean reward of the optimal arm and the mean reward of the ultimately chosen arm. We propose a highly exploring UCB policy and a new algorithm based on successive rejects. We show that these algorithms are essentially optimal since their regret decreases exponentially at a rate which is, up to a logarithmic factor, the best possible. However, while the UCB policy needs the tuning of a parameter depending on the unobservable hardness of the task, the successive rejects policy benefits from being parameter-free, and also independent of the scaling of the rewards. As a by-product of our analysis, we show that identifying the best arm (when it is unique) requires a number of samples of order (up to a log(K) factor) ∑ i 1/∆2i, where the sum is on the suboptimal arms and ∆i represents the difference between the mean reward of the best arm and the one of arm i. This generalizes the well-known fact that one needs of order of 1/∆2 samples to differentiate the means of two distributions with gap ∆.},
edition = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)},
}


@article{torossian_x-armed_2019,
	title = {X-{Armed} {Bandits}: {Optimizing} {Quantiles}, {CVaR} and {Other} {Risks}},
	shorttitle = {X-{Armed} {Bandits}},
	abstract = {We propose and analyze StoROO, an algorithm for risk optimization on stochastic black-box functions derived from StoOO. Motivated by risk-averse decision making fields like agriculture, medicine, biology or finance, we do not focus on the mean payoff but on generic functionals of the return distribution. We provide a generic regret analysis of StoROO and illustrate its applicability with two examples: the optimization of quantiles and CVaR. Inspired by the bandit literature and black-box mean optimizers, StoROO relies on the possibility to construct confidence intervals for the targeted functional based on random-size samples. We detail their construction in the case of quantiles, providing tight bounds based on Kullback-Leibler divergence. We finally present numerical experiments that show a dramatic impact of tight bounds for the optimization of quantiles and CVaR.},
	urldate = {2020-01-13},
	journal = {arXiv:1904.08205 [cs, stat]},
	author = {Torossian, Léonard and Garivier, Aurélien and Picheny, Victor},
	month = oct,
	year = {2019},
	note = {arXiv: 1904.08205},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, quantile bai},
	file = {1904.08205.pdf:/Users/zhangmengyan/Zotero/storage/2NH3WNW6/1904.08205.pdf:application/pdf}
}

@article{howard_sequential_2019,
	title = {Sequential estimation of quantiles with applications to {A}/{B}-testing and best-arm identification},
	abstract = {Consider the problem of sequentially estimating quantiles of any distribution over a complete, fully-ordered set, based on a stream of i.i.d. observations. We propose new, theoretically sound and practically tight confidence sequences for quantiles, that is, sequences of confidence intervals which are valid uniformly over time. We give two methods for tracking a fixed quantile and two methods for tracking all quantiles simultaneously. Specifically, we provide explicit expressions with small constants for intervals whose widths shrink at the fastest possible \${\textbackslash}sqrt\{t{\textasciicircum}\{-1\} {\textbackslash}log{\textbackslash}log t\}\$ rate, as determined by the law of the iterated logarithm (LIL). As a byproduct, we give a non-asymptotic concentration inequality for the empirical distribution function which holds uniformly over time with the LIL rate, thus strengthening Smirnov's asymptotic empirical process LIL, and extending the famed Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to hold uniformly over all sample sizes while only being about twice as wide in practice. This inequality directly yields sequential analogues of the one- and two-sample Kolmogorov-Smirnov tests, and a test of stochastic dominance. We apply our results to the problem of selecting an arm with an approximately best quantile in a multi-armed bandit framework, proving a state-of-the-art sample complexity bound for a novel allocation strategy. Simulations demonstrate that our method stops with fewer samples than existing methods by a factor of five to fifty. Finally, we show how to compute confidence sequences for the difference between quantiles of two arms in an A/B test, along with corresponding always-valid \$p\$-values.},
	urldate = {2020-01-13},
	journal = {arXiv:1906.09712 [math, stat]},
	author = {Howard, Steven R. and Ramdas, Aaditya},
	month = aug,
	year = {2019},
	note = {arXiv: 1906.09712},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Mathematics - Probability, quantile bai, Statistics - Methodology},
	annote = {Comment: 29 pages, 8 figures},
	file = {1906.09712.pdf:/Users/zhangmengyan/Zotero/storage/4BLWD4US/1906.09712.pdf:application/pdf}
}
%-------------------------------
@article{tamkind2019,
  title={Distributionally-Aware Exploration for CVaR Bandits},
  author={Tamkin, Alex and Keramati, Ramtin and Dann, Christoph and Brunskill, Emma},
  year = {2019},
  journal = {NeurIPS}
}

@inproceedings{szorenyi2015qualitative,
  title={Qualitative Multi-Armed Bandits: A Quantile-Based Approach},
  author={Bal{\'a}zs Sz{\"o}r{\'e}nyi and R{\'o}bert Busa-Fekete and Paul Weng and Eyke H{\"u}llermeier},
  booktitle={ICML},
  year={2015}
}


@inproceedings{yu2013sample,
  title={Sample complexity of risk-averse bandit-arm selection},
  author={Yu, Jia Yuan and Nikolova, Evdokia},
  booktitle={Twenty-Third International Joint Conference on Artificial Intelligence},
  year={2013}
}

@inproceedings{honda2014optimality,
  title={Optimality of Thompson sampling for Gaussian bandits depends on priors},
  author={Honda, Junya and Takemura, Akimichi},
  booktitle={Artificial Intelligence and Statistics},
  pages={375--383},
  year={2014}
}
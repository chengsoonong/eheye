

\chapter{Supplementary material to \ourtitle} \label{appA}

\section{Regret Background}
\label{app-sec: Design Choice of Regret}

Depending on the design choice of the summary statistic of reward distributions, the regret can have different design choices.
In this section, we first show the background of regret design choice, namely the mean-based regret \cite{Auer2002, audibert2009exploration, garivier2011kl}, and the risk-averse regret (using the design choices of U-UCB \cite{cassel_general_2018} as an example), then we show the proof of our median-based regret. 
%For each choice, we define the regret and pseudo-regret, following by showing when their expectations are equal and the gap between them. Then we show the decomposition of pseudo-regret. Note in some literature, the regret is also called the cumulative regret.

\subsection{Mean-based Regret}
\label{app-subsec: Mean-based regret}
UCB1 and its variants \cite{Auer2002, audibert2009exploration, garivier2011kl} evaluate the reward distributions by the mean and define the optimal arm as the arm with maximum mean. 

\begin{defi}[Mean-based regret]
\label{defi: mean-based regret}
The mean-based regret is defined as,
\begin{align}
    R^\mu_{N}= \sum_{t=1}^{N} X_{\ast, t}-\sum_{t=1}^{N} \sum_{i = 1}^{K} X_{i, T_i(t)} \mathbb{I}\{A_t = i\}, \nonumber
\end{align}
where 
%$X_{i,T_i(t)}$ is the reward sampled from $P_i$ in round $t$,  $\mathbb{I}\{A_t = i\}$ is the indicator function which returns 1 when $A_t = i$ is true, otherwise returns 0, i.e.  $\sum_{i = 1}^{K} X_{i, T_i(t)} \mathbb{I}\{A_t = i\}$ is the reward sampled at the $t$ round. 
$X_{\ast, t}$ is the reward sample reward from the optimal arm at round $t$.
\end{defi}

\begin{defi}[Mean-based Pseudo-regret]
\label{defi: mean-based pseudo regret}
The pseudo-regret is defined in terms of the true mean of arm distributions,
\begin{align}
    \overline{R}^\mu_{N} =
    N \mu^{*}-\sum_{t=1}^{N} \sum_{i = 1}^{K} \mu_i \mathbb{I}\{A_t = i\}, \nonumber
\end{align}
where $\mu^\ast, \mu_i$ are the mean of optimal arm and arm $i$.
\end{defi}

The difference between the regret and the pseudo-regret comes from the randomness of the rewards. We introduce Wald's Identity in Theorem \ref{theo: wald} and using it to show the expectations of regret and pseudo-regret are the same (Proposition \ref{prop: expectation of regret and pseudo-regret}).

\begin{theo}[Waldâ€™s Identity \cite{wald1944}]
\label{theo: wald}
Let $Y_1, Y_2, Y_3, ...$ be i.i.d. with finite mean, and $N$ is a stopping time with $\mathbb{E}[N] < \infty$, then $\mathbb{E}\left[Y_{1}+\cdots+Y_{N}\right]=\mathbb{E}\left[Y_{1}\right] \mathbb{E}[N]$.
\end{theo}

\begin{prop}
\label{prop: expectation of regret and pseudo-regret}
The expectations of regret and pseudo-regret defined in Definition \ref{defi: mean-based regret} and \ref{defi: mean-based pseudo regret} are equivalent and can be decomposed into the weighted sum of sub-optimal draws. i.e. $\mathbb{E}[R^\mu_N] = \mathbb{E}[\overline{R}^\mu_{N}] = \sum_{i=1}^K \Delta^\mu_i \mathbb{E}[T_i(N)]$, where $\Delta^\mu_i = \mu_\ast - \mu_i$.
\end{prop}

\begin{proof}
\begin{align}
    \mathbb{E}[R^\mu_N] &= \mathbb{E}\left[\sum_{t=1}^{N} X_{\ast, t}-\sum_{t=1}^{N} \sum_{i = 1}^{K} X_{i, T_i(t)} \mathbb{I}\{A_t = i\}\right]\\
\label{proof of expectation of regret UCB1: line 2}
    &= \mathbb{E}\left[\sum_{t=1}^{N} X_{\ast, t}\right]- \mathbb{E}\left[\sum_{t=1}^{N} \sum_{i = 1}^{K} X_{i, T_i(t)} \mathbb{I}\{A_t = i\}\right] \\
\label{proof of expectation of regret UCB1: line 3}
    %& \text{By expectation linearity}\\
    &= \mu_\ast N - \sum_{i = 1}^{K} \mu_i \mathbb{E}[T_i(N)].\\
    %& \text{By Wald's Theorem}\\
    \mathbb{E}[\overline{R}^\mu_N] &= \mathbb{E}\left[N \mu^{*}-\sum_{i = 1}^{K} \mu_i T_i(N)\right]\\
    &= \mu_\ast N - \mathbb{E}\left[\sum_{i = 1}^{K} \mu_i T_i(N)\right] \\
    %& \text{By expectation linearity}\\
    &= \mu_\ast N - \sum_{i = 1}^{K} \mu_i \mathbb{E}\left[T_i(N)\right].
    %& \text{By expectation linearity}
\end{align}

Equation (\ref{proof of expectation of regret UCB1: line 2}) is derived based on the linearity of expectation. By Wald's Identity (Theorem \ref{theo: wald}), we get Equation (\ref{proof of expectation of regret UCB1: line 3}). The derivation of expected pseudo-regret is based on the linearity of expectation.

The pseudo-regret can be decomposed into the weighted sum of sub-optimal draws, where the weight is the gap $\Delta^\mu_i = \mu_\ast - \mu_i$,
\begin{align}
    \overline{R}^\mu_N &= \sum_{i=1}^K \mu_\ast T_i(N) - \sum_{i = 1}^{K} \mu_i T_i(N) \\
    %& \text{By } (\ref{equ: pseudo regret simplified}) \text{ and } \sum_{i = 1}^K T_i(N) = N\\
    %&= \sum_{i=1}^K (\mu_\ast - \mu_i) T_i(N)\\
    &= \sum_{i=1}^K \Delta^\mu_i T_i(N).
\end{align}
Then we can also get the decomposed version of expected regret and pseudo-regret,
\begin{align}
\mathbb{E}[R^\mu_N] = \mathbb{E}[\overline{R}^\mu_N] = \mathbb{E}\left[\sum_{i=1}^K \Delta^\mu_i T_i(N)\right] = \sum_{i=1}^K \Delta^\mu_i \mathbb{E}[T_i(N)].
\end{align}
\end{proof}

Bounding the expected regret is equal to bounding the expected pseudo-regret and is also equal to bounding the expected number of sub-optimal draws.
However, there is still a gap between regret and pseudo-regret. \textcite{coquelin2007bandit} proved that, with probability at least $1 - \beta$, the upper bound of the gap at time $N$ is
\begin{align}
    \left|R^\mu_{N}-\overline{R}^\mu_{N}\right| \leq \sqrt{\sum_{i: m_i < m_\ast} T_i(N) \log (2 / \beta) / 2}.
\end{align}

\subsection{Risk-averse Regret}
\label{app-subsec: Risk-averse Regret}

There is no universal definition for risk-averse regret since different summary statistics of reward distributions are chosen. We show the definition based on \textcite{cassel_general_2018}, where they showed a general approach for bandits under risk criteria, under the class of \textit{Empirical Distribution Performance Measures(EDPM)}. An EDPM evaluates performance by means of a function $U$ , which maps $\hat{F} \text { to } \mathbb{R}$. \textcite{cassel_general_2018} analysed their results based on the weighted sum of distribution and empirical distributions,
\begin{align}
    & F_{N}^{\pi}=\frac{1}{N} \sum_{i=1}^{K} T_{i}(N) F^{(i)},\\
    & \hat{F}_{N}^{\pi}= \frac{1}{N} \sum_{t=1}^{N} \sum_{i = 1}^{K} \hat{F}_{T_i(t)}^{(i)} \mathbb{I}\{A_t = i\},
\end{align}
where $F^{(i)}, \hat{F}_{T_i(t)}^{(i)}$ are the distribution and empirical distribution of arm $i$. By defining the arm with maximum expected distribution measure $U$ as the optimal arm, the regret and pseudo-regret are defined below.

\begin{defi}[Risk-averse Regret]
The risk-averse regret is defined as
\label{defi: regret for cassel}
\begin{align}
    & R_N^{U} =U\left(\hat{F}_{N}^{\pi^{*}(N)}\right)-U\left(\hat{F}_{N}^{\pi}\right)\\
    & = U\left( \frac{1}{N} \sum_{t=1}^N \hat{F}_{T_i(t)}^{\ast} \right)-U\left(\frac{1}{N} \sum_{t=1}^{N} \sum_{i = 1}^{K} \hat{F}_{T_i(t)}^{(i)} \mathbb{I}\{A_t = i\}\right),
\end{align}
where $\hat{F}_{N}^{\pi^{*}(N)}$ is the empirical distribution of the optimal arm.
\end{defi}

\begin{defi}[Risk-averse Pseudo-regret]
The risk-averse pseudo-regret is defined as
 \label{defi: pseudo regret for cassel}
\begin{align}
    & \overline{R}_N^{U} =U\left(F_{p^{*}}\right)-U\left(F_{N}^{\pi}\right)\\
    & = U\left(F^{(\ast)}\right)-U\left(\frac{1}{N} \sum_{i=1}^{K} T_{i}(N) F^{(i)}\right),
\end{align}
where $F_{p^{*}}$ are the distribution of the optimal arm.
\end{defi}

%$\pi^{*}(N) \in \underset{\pi \in \Pi}{\arg \max } \mathbb{E}\left[U\left(\hat{F}_{N}^{\pi}\right)\right]$.

\begin{prop}[\cite{cassel_general_2018}]
When $U$ is linear, the expected regret and expected pseudo-regret defined in Definition \ref{defi: regret for cassel} and \ref{defi: pseudo regret for cassel} are equivalent, and can be decomposed into the weighted sum of sub-optimal draws, i.e. $\frac{1}{N} \sum_{i \neq i^{*}} \mathbb{E}\left[T_{i}(N)\right] \Delta_{i}$, where $\Delta_{i}=U\left(F_{p^{*}}\right)-U\left(F^{(i)}\right)$.
\end{prop}

\begin{proof}
\begingroup
\allowdisplaybreaks
\begin{align}
    &\mathbb{E}[R_N^{U}] =\mathbb{E}\left[U\left(\hat{F}_{N}^{\pi^{*}(N)}\right)-U\left(\hat{F}_{N}^{\pi}\right)\right]\\
\label{proof of expected regret cassel line 2}
     &=U\left(\mathbb{E}\left[\hat{F}_{N}^{\pi^{*}(N)}-\hat{F}_{N}^{\pi}\right]\right) \\
     %& \text{By Jensen's inequality, when U is affine (linear)} \\
\label{proof of expected regret cassel line 3}
     &= U\left(\mathbb{E}\left[\frac{1}{N} \sum_{t=1}^{N} \sum_{i = 1}^{K} \left(\hat{F}_{T_i(t)}^{(i^\ast)} - \hat{F}_{T_i(t)}^{(i)} \right) \mathbb{I}\{A_t = i\}\right]\right)\\
\label{proof of expected regret cassel line 4}
     &= U\left(\frac{1}{N} \sum_{i = 1}^{K} \mathbb{E}\left[\hat{F}_{T_i(t)}^{(i^\ast)} - \hat{F}_{T_i(t)}^{(i)} \right] \mathbb{E}\left[T_i(t)\right]\right)\\
     %& \text{By Wald's Theorem}\\
\label{proof of expected regret cassel line 5}
     &= U\left(\frac{1}{N} \sum_{i = 1}^{K} \left(F_{p^\ast} - F^{(i)}\right) \mathbb{E}\left[T_i(t)\right]\right)\\
     &=\frac{1}{N} \sum_{i = 1}^{K}  U\left(F_{p^\ast} - F^{(i)}\right) \mathbb{E}\left[T_i(t)\right].
     %& \text{Linearity of U}\\
\end{align}

Step (\ref{proof of expected regret cassel line 2}) is derived by Jensen's inequality and U is linear. From (\ref{proof of expected regret cassel line 3}) to (\ref{proof of expected regret cassel line 4}), we use Wald's Identity. Step (\ref{proof of expected regret cassel line 5}) is based on the linearity of U.

\begin{align}
     & \mathbb{E}[\overline{R}_N^{U}]
     = \mathbb{E}\left[U\left(F_{p^{*}}\right)-U\left(F_{N}^{\pi}\right)\right]\\
\label{proof of expected pseudo regret cassel line 2}
     &=U\left(\mathbb{E}\left[F_{p^{*}}-F_{N}^{\pi}\right]\right)\\
     %& \text{By Jensen's inequality, when U is affine (linear)}\\
\label{proof of expected pseudo regret cassel line 3}
      &= U\left(\mathbb{E}\left[\frac{1}{N} \sum_{t=1}^{N} \sum_{i = 1}^{K} \left(F_{p^{*}}-F_{N}^{\pi}\right) \mathbb{I}\{A_t = i\}\right]\right)\\
\label{proof of expected pseudo regret cassel line 4}
     &= U\left(\frac{1}{N} \sum_{i = 1}^{K} \left(F_{p^\ast} - F^{(i)}\right) \mathbb{E}\left[T_i(t)\right]\right)\\
     %& \text{By expectation linearity}\\
\label{proof of expected pseudo regret cassel line 5}
     &=\frac{1}{N} \sum_{i = 1}^{K}  U\left(F_{p^\ast} - F^{(i)}\right) \mathbb{E}\left[T_i(t)\right].
     %& \text{Linearity of U}
\end{align}
\endgroup
Similarly, the Step (\ref{proof of expected pseudo regret cassel line 2}) is derived by Jensen's inequality when U is linear. And the last step is based on the linearity of U.
\end{proof}


\begin{prop}[\cite{cassel_general_2018}]
\label{prop: cassel non liner U}
When $U$ is non-linear but quasiconvex and is strongly stable EDPM (Definition 3 in \cite{cassel_general_2018}), the expected pseudo regret can be bounded by,
        \begin{align}
            \mathbb{E}[\overline{R}^{U}_N] \leq \frac{L}{N} \sum_{i \neq i^{*}} \mathbb{E}\left[T_{i}(N)\right]\left\|F^{\left(i^{*}\right)}-F^{(i)}\right\|,
        \end{align}
where $L=b\left(1+\max _{i, j \in \mathcal{K}}\left\|F^{(i)}-F^{(j)}\right\|^{q-1}\right)$.
\end{prop}

 From Proposition \ref{prop: cassel non liner U} we know, when U is the median of rewards, the expected pseudo-regret can be bounded by the weighted sum of sub-optimal draws, with the weight as $\left\|F^{\left(i^{*}\right)}-F^{(i)}\right\|$. 

\subsection{Median-based Regret}

Instead of defining the regret with respect to empirical distributions, we define the median-based regret and pseudo-regret in Section \ref{subsec: Bounding the Regret}. We show proof for Proposition \ref{prop: expectation of regret and pseudo-regret for median}. 

\EquMedianRegret*

\begin{proof}
\begingroup
\allowdisplaybreaks
\begin{align}
    \mathbb{E}[R_N] 
    %&= \mathbb{E}\left[\sum_{t=1}^N \hat{m}_{\ast, t} - \sum_{t = 1}^N \sum_{i = 1}^K \hat{m}_{i, T_i(t)} \mathbb{I}\{A_t = i\}\right]\\
\label{proof of expectation of regret median: line 2}
    &= \mathbb{E}\left[\sum_{t=1}^{N} \hat{m}_{\ast, t}\right]- \mathbb{E}\left[\sum_{t=1}^{N} \sum_{i = 1}^{K} \hat{m}_{i, T_i(t)} \mathbb{I}\{A_t = i\}\right] \\
\label{proof of expectation of regret median: line 3}
    %& \text{By expectation linearity}\\
    &= m_\ast N - \sum_{i = 1}^{K} m_i \mathbb{E}[T_i(N)].\\
    %& \text{By Wald's Theorem}\\
    \mathbb{E}[\overline{R}_N]
    %&= \mathbb{E}\left[N m_\ast - \sum_{i=1}^K m_i T_i(N)\right]\\
    &= m_\ast N - \mathbb{E}\left[\sum_{i = 1}^{K} m_i T_i(N)\right] \\
    %& \text{By expectation linearity}\\
    &= m_\ast N - \sum_{i = 1}^{K} m_i \mathbb{E}\left[T_i(N)\right].
    %& \text{By expectation linearity}
\end{align}
Equation (\ref{proof of expectation of regret median: line 2}) is derived based on the linearity of expectation. By Wald's Identity (Theorem \ref{theo: wald}), we get Equation (\ref{proof of expectation of regret median: line 3}). The derivation of expected pseudo-regret is based on the linearity of expectation.

The pseudo-regret for median can be decomposed into the weighted sum of sub-optimal draws, where the weighted is the gap $\Delta_i = m_\ast - m_i$,

\begin{align}
    \overline{R}_N &= \sum_{i=1}^K m_\ast T_i(N) - \sum_{i = 1}^{K} m_i T_i(N) \\
    %& \text{By } (\ref{equ: pseudo regret simplified}) \text{ and } \sum_{i = 1}^K T_i(N) = N\\
    &= \sum_{i=1}^K (m_\ast - m_i) T_i(N)\\
    &= \sum_{i=1}^K \Delta_i T_i(N)
\end{align}
\endgroup
Then we can also get the decomposed version of expected regret and pseudo-regret,
\begin{align}
\mathbb{E}[R_N] = \mathbb{E}[\overline{R}_N] = \mathbb{E}\left[\sum_{i=1}^K \Delta_i T_i(N)\right] = \sum_{i=1}^K \Delta_i \mathbb{E}[T_i(N)],
\end{align}
\end{proof}


\section{Experiment Design and Report}
\label{app-sec: Experiment Design and Report} 

In this section, we show empirical considerations for our policy and briefly explain the benchmark algorithms used in Section \ref{sec: Empirical Experiments}. 

\subsection{Empirical Considerations}
\label{app-sec: Empirical Considerations}

We cover two empirical aspects of our policy, namely tuning hyper-parameters and hazard rate lower bound estimation.

There are two hyper-parameters in Algorithm \ref{alg:Policy for IHR distributions}: $\alpha, \beta$, which can be tuned empirically to best serve the reward distributions. Depending on different reward distributions, the level of exploration can be adjusted in terms of the empirical experiment. Intuitively, if the rewards distributions have relatively small variance or the gap of median rewards is large, then less exploration is needed, i.e. small $\alpha, \beta$ could lead to better performance.

Algorithm \ref{alg:Policy for IHR distributions} includes the lower bound of hazard rate of the reward distributions. For empirical experiment, we need to estimate the $L_{i,T_i(t)}$.
Assume reward distribution is defined on $[0, \infty)$ and non-decreasing, then for a small amount $\de t$, the lower bound can be estimated by $L = h(0) = f(0)/ (1- F(0)) \approx f(0)  = \frac{\mathbb{P}\left(0 < X < 0 + \de t\right)}{dt}$.


\subsection{Benchmark Algorithms}
\label{app-subsec: Benchmark Algorithms}

% The expected regrets of these algorithms are defined based on different weights of sub-optimal draws. And bounding the regret can be reduced to bounding the sub-optimal draws,  we thus compare the expected sub-optimal draws $\sd$ instead of expected regret.

 We first specify the benchmark algorithms used for empirical evaluations, then we show the parameter choices.  

 \begin{itemize}
     \item UCB1 \cite{Auer2002}.\\
    With the assumption of the reward support bounded within [0,b], for arm $i$ at round $t$, UCB1 pick the arm with index
     \begin{align}
         \argmax_{i \in \mathcal{K}} \hat{\mu}_{i, T_i\left(t-1\right)} + \sqrt{\frac{2b^2 \log t}{T_i\left(t-1\right)}},
     \end{align}
     where $\hat{\mu}_{i, T_i\left(t-1\right)}$ is the empirical mean for arm $i$ at the round $t-1$. The policy is derived based on Hoeffding inequality. 
     \item UCB-V \cite{audibert2009exploration}.\\
     Inspired by the idea ``variance-aware" algorithm that works for upper confidence bound policies, \textcite{audibert2009exploration} proposed UCB-V, using empirical mean to summarise central tendency and using variance estimate in its confidence width. The confidence width part is
\begin{align}
\label{policy ucbv}
    \sqrt{\frac{2 V_{i, T_{i}\left(t-1\right)} \varepsilon_{T_{i}\left(t-1\right), t}}{T_{i}\left(t-1\right)}}+c \frac{3 b \varepsilon_{T_{i}\left(t-1\right), t}}{T_{i}\left(t-1\right)},
\end{align}
where $V_{k,s}$ is the empirical variance estimate for arm $i$ based on $s$ samples, $\varepsilon$ is the exploration function and a typical choice is $\zeta \log t$. The hyper-parameter $\zeta, c >0$ can be tuned and controls the algorithm's behaviour. 
The policy is derived from a Bernstein type tail bound on the empirical mean of i.i.d. random variables with bounded support.
\textcite{audibert2009exploration} also did a risk analysis by looking at the tail distribution of regret.  

     \item MV-LCB \cite{sani_risk-aversion_2012}.\\
     \textcite{sani_risk-aversion_2012} proposed the MV-LCB algorithm based on the principle of risk-aversion with the objective of best risk-return balance. Using variance as a measure of risk, they proposed to use mean-variance model and pick an arm with index
\begin{align}
\argmin_{i \in \mathcal{K}} \widehat{\mathrm{MV}}_{{i}, T_{i, t-1}}-(5+\rho) \sqrt{\frac{\log 1 / \delta}{2 T_{i, t-1}}},
\end{align}
where $\widehat{\mathrm{MV}}_{{i}, T_{i, t-1}}=\hat{\sigma}_{{i}, T_{i, t-1}}^{2}-\rho \hat{\mu}_{{i}, T_{i, t-1}}$, where $\hat{\mu}_{{i}, T_{i, t-1}}, \hat{\sigma}_{{i}, T_{i, t-1}}^{2}$ are empirical mean and variance, $\rho$ is the value of risk tolerance.

The policy is derived from the high-probability confidence bounds on empirical mean-variance of i.i.d. random variables with bounded support based on Chernoff-Hoeffding inequality.

     \item Exp3 \cite{Auer2003adv}.\\
Exp3 is designed for adversarial bandits, with no statistical assumption of the process generating the rewards of arms. The adversarial bandit algorithms intuitively suit the situation where the process of generating rewards includes outliers. On each step $t$, Exp3 draws an arm according to the distribution $p_{1}(t), \ldots, p_{K}(t)$, which is set to
\begin{align}
    p_{i}(t)=(1-\gamma) \frac{w_{i}(t)}{\sum_{j=1}^{K} w_{j}(t)}+\frac{\gamma}{K},
\end{align}
with $\gamma \in(0,1]$ and updated parameters for $j = 1, ..., K$,
\begin{align*}
    &\hat{x}_{j}(t) =\left\{\begin{array}{cc}{x_{j}(t) / p_{j}(t)} & {\text { if } j=i_{t}} \\ {0} & {\text { otherwise }}\end{array}\right.\\
    & w_{j}(t+1)=w_{j}(t) \exp \left(\gamma \hat{x}_{j}(t) / K\right)
\end{align*}
Note that instead of a logarithmic expected regret bound, Exp3 yields an expected regret of $O(\sqrt{K N \ln K})$. Exp3 assume the support of reward distributions is bounded. 

 \end{itemize}
 
 Parameters are chosen with the best performance for each algorithm by grid searching. 
 For simulated experiment, the parameters are tuned as following, UCB1 with $b = 1$, UCB-V with $c = 1, \zeta = 1.2, b = 0.5$, MV-UCB with $\rho = 50$, Exp3 with $\gamma = 0.5$.
 For clinical experiment, the parameters are tuned as following, UCB1 with $b = 100$, UCB-V with $c = 1, \zeta = 1.2, b = 100$, MV-UCB with $\rho = 50$ and $1e8$, Exp3 with $\gamma = 0.3$.

\section{Comparison on Concentrations}
\label{sec: Comparison with Related Work}

In this section, we compare our proposed concentration inequalities and assumptions with related works.
To design our M-UCB policy, we proved a Bernstein like concentration inequality for quantiles (Theorem \ref{theo: Bernstein Inequality for Quantiles.}), which resembles the Bernstein inequality as following.

\begin{theo} [Bernstein Inequality \cite{bernstein1924modification}]
\label{theo: Bernstein Inequality}
Let $X_{1} ... X_{n}$ be random variables which independently distribute according to a distribution $F$, satisfying $\mathbb{E}[(X_i - \mathbb{E}[X_i])^2] \leq v$, where $v$ is the variance factor, and ${||X_i - \mathbb{E}[X_i]||}_{\infty} \leq M$ for all $i = 1, ..., n$. Then for $\varepsilon > 0$, 
\begin{align}
    \mathbb{P}(\frac{1}{n} \sum_{i = 1}^n (X_i - \mathbb{E}[X_i]) \geq \sqrt{\frac{2 v \varepsilon}{n}}+ \frac{2 M \varepsilon}{3n}) \leq e^{-\varepsilon}.
\end{align}
\end{theo}

Bernstein inequality is an important concentration inequality for the sum of independent random variables, which releases the bounded support assumption and only assumes finite variance. When variance is small enough, the confidence width decrease with the fast rate $\frac{1}{n}$. Otherwise, the small rate $\frac{1}{\sqrt{n}}$ dominates the convergence.

Recall in Theorem \ref{theo: Bernstein Inequality for Quantiles.}, we propose a concentration inequality for quantiles, which has a similar form of the original Bernstein inequality. Unlike Bernstein inequality shown in Theorem \ref{theo: Bernstein Inequality}, our proposed inequality shows concentration for medians instead of sum of random variables.

\BernQuant*

when plugging in $v_n$, and let $q = \frac{1}{2}$, the concentration inequality can be expressed as 
\begin{align}
    \mathbb{P}\left(\hat{m} - \mathbb{E}[\hat{m}] \geq \frac{2 \sqrt{\varepsilon}}{L}\left(\sqrt{\frac{2}{n}} + \sqrt{\varepsilon} \frac{2}{n}\right)\right) \leq  e^{- \varepsilon},
\end{align}
a topical choice of $\varepsilon$ is $\alpha log t$, where $t$ is the current round, $\alpha$ is the hyper-parameter. i.e.
\begin{align}
    \mathbb{P}\left(\hat{m} - \mathbb{E}[\hat{m}] \geq \frac{2 \sqrt{\alpha \log t}}{L}\left(\sqrt{\frac{2}{n}} + \sqrt{\alpha \log t} \frac{2}{n}\right)\right) \leq  e^{- \varepsilon}.
\end{align}
At the beginning, $\alpha$ balances the tradeoff between the fast rate ($O(\frac{1}{n})$) and slow rate ($O(\frac{1}{\sqrt{n}})$) of convergence. As $t$ increases, the fast rate dominates the convergence. Note our convergence rate does not reply on the value of variance.  

 Compared with related works, the concentration inequality of our work also allows unbounded reward support and does not directly depend on the density around $\hat{m}$. 
This theoretically guarantees the logarithmic rate of regret and sub-optimal of bandit task when reward distributions satisfied Assumption \ref{ass:IHR}.


Concentration inequality for quantiles (or medians) has been proposed by other work as well. \textcite{cassel_general_2018} proposed the general framework for bandit under risk criteria.
%Instead of evaluating the performance using the cumulative rewards, they propose to use the expected performance $\mathbb{E} U\left(\hat{F}\right)$, where $U$ is a performance criterion function mapping the empirical reward distribution to a real-valued number. For example, $U^{a v e}\left(\hat{F}\right)=\frac{1}{n} \sum_{s=1}^{n} X_{s}$ is the empirical mean. See Appendix \ref{app-subsec: Risk-averse Regret} for the regret defined based on $U$.
%$\tilde{U}^{C V a R_{\alpha}}\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{[n \alpha]} \sum_{s=1}^{\lceil n \alpha\rceil} X_{(s)}$ is the empirical CVaR, where $X_{(s)}$ is the $s^{th}$ order statistics of $\left(X_{1}, \ldots, X_{n}\right)$.
%When $U$ is the empirical median in \cite{cassel_general_2018},
For $\varepsilon > 0$, the tail probability for empirical median is
\begin{align}
    \mathbb{P}\left(\hat{m} - \mathbb{E}[\hat{m}] \geq 2b \sqrt{\frac{\log 4n + \varepsilon}{n}}\right) \leq  e^{- \varepsilon},
\end{align}
which is derived based on Hoeffding inequality and requires bounded reward support.
\textcite{kolla_concentration_2019} proved the concentration inequality for quantiles based on the Dvoretzkyâ€“Kieferâ€“Wolfowitz (DKW) inequality. Especially, for any $\varepsilon >0$,  the tail probability bound for empirical medians is 
\begin{align}
\label{equ: Concentration inequality for quantiles based on DKW}
    P \left(\hat{m} - \mathbb{E}[\hat{m}] \geq \sqrt{\frac{\varepsilon}{2nc}}\right) \leq  e^{- \varepsilon},
\end{align}
where $c$ is a constant that depends on the value of the density $f$ of $X$ in a neighbourhood of $\hat{m}$.
Note that this bound depends on the value of $c$. When $c$ approaches 0, the bound is useless. For both of the works, the confidence width decays with a slow rate.  




\section{Special Case Distributions for IHR}
\label{app-sec: special case}

In this section, we specify two special cases for IHR assumption, the sub-optimal bounds of which are shown in Table \ref{table: Summary for policies and bounds.} in Section \ref{subsec: Bounding the Regret}.

\begin{defi}[Absolute Gaussian Distribution]
\label{defi: AbsGau}
Given a Gaussian random variable $X$ with mean $\mu$ and variance $\sigma^2$, the random variable $Y = |X|$ has a absolute Gaussian distribution with the PDF $\tilde{\phi}$ and CDF $\tilde{\Phi}$,
\begin{align}
    \tilde{\phi} = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}+\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x+\mu)^{2}}{2 \sigma^{2}}},\\
    \tilde{\Phi} = \frac{1}{2}\left[\operatorname{erf}\left(\frac{x+\mu}{\sigma \sqrt{2}}\right)+\operatorname{erf}\left(\frac{x-\mu}{\sigma \sqrt{2}}\right)\right],
\end{align}
where the error function $\text{erf}\left(x\right)= \frac{1}{\sqrt{\pi}} \int_{-x}^{x} e^{-t^2} dt$. We denote the absolute Gaussian distribution random variable with mean $\mu$ and variance $\sigma^2$ as $|\mathcal{N}(\mu, \sigma^2)|$.
\end{defi}

\begin{defi} [Exponential Distribution]
\label{defi: Exp}
For $\theta > 0$, PDF and CDF of exponential distribution:
\begin{align}
    \label{Expon PDF}
    f_{Exp}\left(x, \theta\right) &= \theta e^{-\theta x},\\
    \label{expon CDF}
    F_{Exp}\left(x, \theta\right) &= 1 - e^{-\theta x},
\end{align}
We denote the exponential distribution with $\theta$ as $Exp(\theta)$.
\end{defi}




\section{Proof}
\label{app-sec: Proof}

In this section, we show proofs of the theorems in Section \ref{sec: Theorem}. 

\subsection{Proof for Proposition \ref{prop: bound of expected spacing}}

To prove Proposition \ref{prop: bound of expected spacing}, we first show R-transform and R\'enyi's representation of order statistics.

\begin{defi} [R-transform]
\label{defi: R-transform}
The R-transform of a distribution F is defined as the non-decreasing function on $[0, \infty)$ by $R(t) = \inf\{x: F(x) \geq 1 - e^{-t}\} = F^{-1}(1-e^{-t})$.
\end{defi}
Observe that the R-transform defined in Definition \ref{defi: R-transform} is actually the quantile transformation with respect to the CDF of standard exponential distribution, i.e. $F^{-1}\left(F_{exp}\left(t\right)\right)$.

\begin{theo} [\textit{R\'enyi's representation, Theorem 2.5 in \cite{boucheron2012}}]
\label{theo: Renyi's representation}
Let $X_{\left(1\right)} \geq \ldots \geq X_{\left(n\right)}$ be the order statistics of samples from distribution F, $Y_{\left(1\right)} \geq Y_{\left(2\right)} \geq \ldots \geq Y_{\left(n\right)}$ be the
order statistics of independent samples of the standard exponential distribution, then
\begin{align}
{\scriptstyle
    \left(Y_{\left(n\right)}, \ldots, Y_{\left(i\right)}, \ldots, Y_{\left(1\right)}\right)  \stackrel{d}{=} \left(\frac{E_{n}}{n}, \ldots, \sum_{k=i}^{n} \frac{E_{k}}{k}, \ldots, \sum_{k=1}^{n} \frac{E_{k}}{k}\right),
}
\end{align}
where $E_{1}, \ldots, E_{n}$ are independent and identically distributed (i.i.d.) standard exponential random variables, and
\begin{align}
    \left(X_{\left(n\right)}, \ldots, X_{\left(1\right)}\right) \stackrel{d}{=} \left(R \left(Y_{\left(n\right)}\right), \ldots, R \left(Y_{\left(1\right)}\right)\right),
\end{align}
where $R\left(\cdot\right)$ is the R-transform defined in Definition \ref{defi: R-transform}, equality in distribution is denoted by $\stackrel{d}{=}$.
\end{theo}

The R\'enyi's representation shows the order statistics of an Exponential distribution are linear combinations of independent Exponentials, which can be extended to the representation for order statistics of a general continuous $F$ by quantile transformation using R-transform. 

The following proposition states the connection between the \textit{IHR} and R-transform.

\begin{prop} [Proposition 2.7 \cite{boucheron2012}]
\label{prop non-increasing hazard rate}
Let F be an absolutely continuous distribution function with hazard
rate h, the derivative of R-transform is $ R^{\prime}=1 / h\left(R\right)$. Then if the hazard rate h is non-decreasing, then for all $t > 0$ and $x > 0$, $R\left(t+x\right)-R(t) \leq x / h\left(R(t)\right).$
\end{prop}
Based on the above proposition, the expectation of spacing can be bounded as shown in Proposition \ref{prop: bound of expected spacing}.

\BoundExpSpacing*
\begin{proof}
%We can bound the expectations of the $k^{th}$ spacing of order statistics, assuming the lower bound hazard rate is L,
\begin{align}
    \label{equ: lemma 1 proof 1}
    \mathbb{E}[S_k]
    &= \mathbb{E}[X_{(k)} - X_{(k + 1)}]\\
    \label{equ: lemma 1 proof 2}
    &= \mathbb{E}[R\left(Y_{\left(k+1\right)} + \frac{E_k}{k}\right) - R\left(Y_{\left(k+1\right)}\right)]\\
    \label{equ: lemma 1 proof 3}
    &= \int_{Y} \int_{E} \left(R\left(y + \frac{z}{k}\right) - R\left(y\right)\right) f_Y\left(y\right) f_E\left(z\right) \de z \de y \\
    & \leq \int_{Y} \int_{E} \frac{z}{k \times h\left(R\left(y\right)\right)} f_Y\left(y\right) f_E\left(z\right) \de z \de y \\
    \label{equ: lemma 1 proof 4}
    & \leq \int_{E} \frac{z}{kL} f_E\left(z\right) \de z\\
    &= \frac{1}{kL}.
\end{align}
Equation (\ref{equ: lemma 1 proof 1}) to (\ref{equ: lemma 1 proof 2}) follows the R\'enyi's Representation (Theorem \ref{theo: Renyi's representation}), where $E_{k}$ is standard exponentially distributed and independent of $Y_{\left(k + 1\right)}$.
Equation (\ref{equ: lemma 1 proof 3}) follows the definition of expectation and we denote the value of random variables by lower case letters and the random variables by upper case letters, i.e. $z, y$ is the value of random variable $E_{k}, Y_{(k+1)}$.  
Equation (\ref{equ: lemma 1 proof 3}) to (\ref{equ: lemma 1 proof 4}) follows the Proposition \ref{prop non-increasing hazard rate} under the assumption of IHR and with $L$ as the lower bound of hard rate $h\left(R\left(y\right)\right)$.
\end{proof}

\subsection{Proof of Lemma \ref{lemma: bound the upper bound of expon efron-stein}}
\BoundUpperExpEfronStein*
\begin{proof}
From Theorem \ref{theo: Renyi's representation}, we can represent the spacing as $S_{k}=X_{\left(k\right)}-X_{\left(k + 1\right)} \stackrel{d}{=} R\left(Y_{\left(k+1\right) + E_{k} /k} \right)-R\left(Y_{\left(k+1\right)}\right)$, where $E_{k}$ is standard exponentially distributed and independent of $Y_{\left(k + 1\right)}$. Let $v_n = \frac{n}{k^2 L^2}$,
\begin{align}
    \label{equ:Theo 4 proof 1}
    & \lambda\mathbb{E}\left[S_{k}\left(e^{\lambda S_{k}}-1\right)\right] \\
    \label{equ:Theo 4 proof 2}
    \leq & \lambda  \int_{E} \int_{Y} \frac{z}{h\left(R\left(y\right)\right)k} \left(e^{ \frac{\lambda z}{h\left(R\left(y\right)\right)k}} - 1\right) \nonumber\\
    & f_Y\left(y\right) f_E\left(z\right) \de y \de z\\
    \label{equ:Theo 4 proof 3}
    \leq &  \int_E \lambda  \sqrt{\frac{v_{n}}{n}} z \left(e^{\lambda \sqrt{\frac{v_{n}}{n}}z} -1\right) f_E\left(z\right) \de z \\
    \label{equ:Theo 4 proof 4}
    =  & \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} z \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} z } -1\right) e^{-z} \de z\\
    \label{equ:Theo 4 proof 5}
    \leq &  \frac{2\lambda^2 v_n}{n \left(1 - 2 \lambda \sqrt{\frac{v_n}{n}}\right)}.
\end{align}
Similar as the proof of Lemma \ref{prop: bound of expected spacing}, Equation (\ref{equ:Theo 4 proof 1}) to (\ref{equ:Theo 4 proof 2}) follows Proposition \ref{prop non-increasing hazard rate} with IHR assumption, and from Equation (\ref{equ:Theo 4 proof 2}) to (\ref{equ:Theo 4 proof 3}), we assume the lower bound of hazard rate is $L$. The last step is because for $0 \leq \mu \leq \frac{1}{2}$, $\int_{0}^{\infty} \mu z\left(e^{\mu z}-1\right) e^{-z} \mathrm{d} z=\frac{\mu^{2}\left(2-\mu\right)}{\left(1-\mu\right)^{2}} \leq \frac{2 \mu^{2}}{1-2 \mu}$.
\end{proof}{}

\subsection{Proof for Theorem \ref{theo: Bernstein Inequality for Quantiles.}}
\label{app-sec: proof for theo: Bernstein Inequality for Quantiles.}

\BernQuant*

\begin{proof}
Considering the quantile case for Lemma \ref{lemma: bound the upper bound of expon efron-stein}, let $S_{qn} = X_{(qn)} - X_{(qn + 1)}$ and $v_n = \frac{1}{q^2 n L^2}$, then
\begin{align}
    \lambda\mathbb{E}\left[S_{qn}\left(e^{\lambda S_{qn}}-1\right)\right] \leq  \frac{2\lambda^2 v_n}{n \left(1 - 2 \lambda \sqrt{\frac{v_n}{n}}\right)}.
\end{align}{}


Then from Theorem \ref{theo: Exponential Efron-Stein inequality}, for $0 < q \leq \frac{1}{2}$,
\begin{align}
    & \log \mathbb{E} e^{\lambda\left(X_{\left(qn\right)}-\mathbb{E}[X_{\left(qn\right)}] \right)} \nonumber \\
    \leq& \lambda \frac{qn}{2} \mathbb{E}\left[S_{qn}\left(e^{\lambda S_{qn}}-1\right)\right]\\
    \leq&
    \frac{q \lambda^{2} v_{n}}{\left(1-2 \lambda \sqrt{v_{n} / n}\right)}\\
    \leq&
    \frac{ \lambda^{2} v_{n}}{2 \left(1-2 \lambda \sqrt{v_{n} / n}\right)}.
\end{align}
Inequality (\ref{equ: log mgf for quantile}) is thus proved. 
%Such a random variable satisfies a so-called Bernstein inequality \cite{boucheron2013}. 
To equivalently express the logarithmic moment generating function bound into a tail probability bound, we make use of the Cram\'er-Chernoff method \cite{boucheron2013}. Markov's inequality implies, for $\lambda > 0$,
\begin{align}
    \mathbb{P}(Z \geq t) \leq e^{-\lambda t} \mathbb{E}[e^{\lambda Z}],
\end{align}
where $Z = X_{\left(qn\right)}-\mathbb{E}[X_{\left(qn\right)}]$. To choose $\lambda$ to minimise the upper bound, one can introduce $\psi_{Z}^{*}(t)=\sup _{\lambda \geq 0}\left(\lambda t-\psi_{Z}(\lambda)\right)$, with the $\psi_{Z}(\lambda)$ being the moment generating function, i.e. $\psi_{Z}(\lambda)=\log \mathbb{E}[e^{\lambda Z}]$. Then we get $\mathbb{P}(Z \geq t) \leq \exp \left(-\psi_{Z}^{*}(t)\right)$. Thus, for $\varepsilon > 0$,
\begin{align}
    \mathbb{P}\left(X_{\left(qn\right)}-\mathbb{E}[X_{\left(qn\right)}] \geq \sqrt{2 v_{n} \varepsilon}+2 \varepsilon \sqrt{v_{n} / n}\right) \leq e^{-\varepsilon}.
\end{align}

\end{proof}

\subsection{Proof of Theorem \ref{theo: sub-optimal draws bound}.}
\label{app-subsec: proof of theo: sub-optimal draws bound}


The proof of Theorem \ref{theo: sub-optimal draws bound} is based on three Lemmas shown as following. The proof structure is based on \cite{Auer2002}. We set $\beta = 1$ for the following proof.
\begin{lemma}
\label{lemma: appendix sub-optimal bound proof 1}
Let $l$ be an arbitrary positive integer, $\hat{m}_{*, s}$ is the empirical median of the reward samples of the optimal arm (i.e. the arm with maximum median) when it has been played s times, similarly $\hat{m}_{i, s_i}$ is the empirical median of the reward samples of arm $i$ when it has been played $s_i$ times. $D_i(t, s_i)$ is the confidence width defined in Section \ref{sec: policy}. For $N \geq 1$, the number that arm $i$ is chosen is bounded by
\begin{align}
    T_i\left(N\right) \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \nonumber \\
    & \{\hat{m}_{*, s} + D_*(t, s)  \leq \hat{m}_{i, s_i} + D_i(t, s_i)\}.
\end{align}
\end{lemma}

\begin{proof}
Let $A_t$ represent the arm drawn in the round $t$, assume there are $k$ arms and each arm is initialised once. Then the number of times arm $i$ has been drawn up to round $N$ is
\begingroup
\allowdisplaybreaks
    \begin{align}
        & T_i\left(N\right) \nonumber\\
        =& 1 + \sum_{t = k + 1}^n \{A_t = i\}\\
\label{proof: second line in proof of lemma: bound the number of draws}
       \leq & l + \sum_{t = k + 1}^n \left(\{A_t = i\} \cup \{T_i\left(t-1\right) \geq l\}\right)\\
       \label{proof: one step before minmax for normal}
       \leq & l + \sum_{t = k + 1}^n (\{ \hat{m}_{*, T_*\left(t-1\right)} +  D_*\left(t, T_*\left(t-1\right)\right) \leq \nonumber\\
       &\hat{m}_{i, T_i\left(t-1\right)} +  D_i\left(t, T_i\left(t-1\right)\right)\} \cup \{ T_i\left(t-1\right) \geq l\})\\
       \label{proof: minmax for normal}
       \leq & l + \sum_{t = k + 1}^n \{ \min \limits_{0 < s < t}\hat{m}_{*, s} + D_*(t, s) \nonumber \\
       & \leq \max \limits_{l < s_i < t}\hat{m}_{i, s_i} +  D_i(t, s_i)\}\}\\
       \label{proof: union bound for normal}
       \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{m}_{*, s} + D_*(t, s)  \leq \hat{m}_{i, s_i} +  D_i(t, s_i)\}.
    \end{align}

    where the symbols with $*$ representing the properties of the optimal arm (i.e. the arm with maximum median).
    From step (\ref{proof: second line in proof of lemma: bound the number of draws}) to step (\ref{proof: one step before minmax for normal}), we make use of the fact that arm $i$ is selected only when the upper confidence width for arm $i$ constructed by M-UCB is larger than or equal to the one for the optimal arm.
    From step (\ref{proof: one step before minmax for normal}) to step (\ref{proof: minmax for normal}), we make use of the fact $l \leq T_i\left(t-1\right) < t$ and $0 < T_*\left(t-1\right) < t$. From step (\ref{proof: minmax for normal}) to (\ref{proof: union bound for normal}), we use the union bound. And in the step (\ref{proof: union bound for normal}), t is summed from 1 to infinity to make sure the arm $i$ can be played for s times.\\
\endgroup
\end{proof}

\begin{lemma}
\label{lemma: appendix sub-optimal bound proof 2}

Let $\hat{m}_{*, s}$ be the empirical median of the reward samples of the optimal arm (i.e. the arm with maximum median) when it has been played s times, Similarly, $\hat{m}_{i, s_i}$ is the empirical median of the reward samples of arm $i$ when it has been played $s_i$ times, where $s \geq 1, s_i \geq l$, l is an arbitrary integer,  $D_i(t, s_i)$ is the confidence width defined in Section \ref{sec: policy}.

    $\hat{m}_{*, s} +   D_*(t, s)  \leq \hat{m}_{i, s_i} +   D_i(t, s_i)$ implies that at least one of the following must hold
    \begin{align}
        \label{lemma 3.1}
        \hat{m}_{*, s} +   D_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}],\\
        \label{lemma 3.2}
        \hat{m}_{i, s_i} -   D_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}],\\
        \label{lemma 3.3}
        \mathbb{E}[\hat{m}_{*, s}] < \mathbb{E}[\hat{m}_{i, s_i}] + 2   D_i(t, s_i).
    \end{align}
\end{lemma}

\begin{proof}
Assume all of the three inequalities are not true, then we have
    \begin{align}
        \label{proof lemma 3.1}
         \hat{m}_{*, s} +   D_*(t, s) >  \mathbb{E}[\hat{m}_{*, s}],\\
        \label{proof lemma 3.2}
        \hat{m}_{i, s_i} -   D_i(t, s_i) < \mathbb{E}[\hat{m}_{i, s_i}],\\
        \label{proof lemma 3.3}
        \mathbb{E}[\hat{m}_{*, s}] \geq \mathbb{E}[\hat{m}_{i, s_i}] + 2   D_i(t, s_i).
    \end{align}
    (\ref{proof lemma 3.1}) - (\ref{proof lemma 3.2}) we get,
    \begin{align}
         & \mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}] \nonumber\\
         < & \hat{m}_{*, s} +   D_*(t, s) - \left(\hat{m}_{i, s_i} -   D_i(t, s_i)\right) \\
        \leq & \hat{m}_{i, s_i} +   D_i(t, s_i) - \left(\hat{m}_{i, s_i} -   D_i(t, s_i)\right) \\
        = & 2   D_i(t, s_i),
    \end{align}
    which contradicts (\ref{proof lemma 3.3}), the assumption that all of the three inequalities are not true doesn't hold. Lemma \ref{lemma: appendix sub-optimal bound proof 2} is proved to be true.\\
\end{proof}

\begin{lemma}
\label{lemma: upper bound of expected number of draws}
Let $\hat{m}_{*, s}$ is the empirical median of the reward samples of the optimal arm (i.e. the arm with maximum median) when it has been played s times, similarly $\hat{m}_{i, s_i}$ is the empirical median of the reward samples of arm $i$ when it has been played $s_i$ times. $D_i(t, s_i)$ is the confidence width defined in Section \ref{sec: policy}. 
Define $l' = \Bigl\lceil \frac{C_{i} \log N}{(\Delta_i L_{i})^2}  \Bigr\rceil,$ where $ C_{i} = 32 \left[(2 + \Delta_i L_{i}) + 2\sqrt{ 1 + \Delta_i L_{i}}\right]$.
 For $N \geq 1$, the expected number that arm $i$ is chosen is bounded by
\begin{align}
\label{equ: proof upper bound of expected number of draws}
    \mathbb{E}[T_i\left(N\right)] &\leq l' + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l'}^{t-1} \mathbb{P}\left(\hat{m}_{*, s} +  D_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]\right)\nonumber \\ & + \mathbb{P}\left(\hat{m}_{i, s_i} -  D_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}]\right).
\end{align}
\end{lemma}

\begin{proof}
To give a bound for $\mathbb{E}[T_i\left(N\right)]$, from Lemma \ref{lemma: appendix sub-optimal bound proof 1}, we only need to give a bound of $\hat{m}_{*, s} +  D_*(t, s)  \leq \hat{m}_{i, s_i} +  D_i(t, s_i)$. From Lemma \ref{lemma: appendix sub-optimal bound proof 2} we know, if we make (\ref{lemma 3.3}) false, then
\begin{align}
    & \mathbb{P}\left(\hat{m}_{*, s} +  D_*(t, s)  \leq \hat{m}_{i, s_i} +  D_i(t, s_i)\right) \\
    \leq &  \mathbb{P}\left(\hat{m}_{*, s} +  D_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]\right) + \nonumber \\ & \mathbb{P}\left(\hat{m}_{i, s_i} -  D_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}]\right).
\end{align}
According to Corollary \ref{theo: Bernstein Inequality for Medians.}, we can have the two-side bound for medians in (\ref{lemma 3.1})(\ref{lemma 3.2}) as
    \begin{align}
        & \mathbb{P}\left(\hat{m}_{*, s} +  D_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]\right)\\ \leq & \mathbb{P}\left(\hat{m}_{i, s_i} -  D_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}]\right)\\
        \leq &  e^{-\alpha \log t} = t^{-\alpha}.
    \end{align}
Note the left tail bound is not sharp in this case.

    Now we only need to find the value of $l$ to make (\ref{lemma 3.3}) false, i.e.
    \begin{align}
    \label{goal to find l}
        D_i(t, s_i) \leq  \frac{1}{2 }\left(\mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}]\right).
    \end{align}
    Remind that $\Delta_{i} = m_* - m_i = \mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}]$, then (\ref{goal to find l}) can be written as $D_i(t, s_i) \leq  \frac{\Delta_{i}}{2 }$. With $t \leq N, 1 \leq l < s_i, \alpha = 4$,
    \begin{align}
        D_i(t, s_i) &= \frac{4 \sqrt{\log t} \left( \sqrt{ 2s_i} + 4\sqrt{\log t}\right)}{s_i L_{i}}\\
        & \leq \frac{4 \sqrt{\log N}}{ L_{i}} \times \frac{\sqrt{2s_i} + 4\sqrt{\log N}}{s_i},\\
    \end{align}
    let $ C_{i} = 32 \left[(2 + \Delta_i L_{i}) + 2\sqrt{1 + \Delta_i L_{i}}\right]$,
    \begin{align}
        s_i \geq \frac{ C_{i} \log N}{(\Delta_i L_{i})^2}.
    \end{align}

    Then we have $l'$ which makes (\ref{lemma 3.3}) false,
   \begin{align}
       l' = \Bigl\lceil \frac{ C_{i} \log N}{(\Delta_i L_{i})^2}  \Bigr\rceil,
   \end{align}
   with $l'$, (\ref{equ: proof upper bound of expected number of draws}) is proved. 
 \end{proof}
   
Now, we are ready to prove Theorem \ref{theo: sub-optimal draws bound} based on the above three lemmas. The proof is shown as follows.

\SubOptDrawsBound*

\begin{proof}

    We get the bound of $\mathbb{E}[T_i\left(N\right)]$ based on Lemma \ref{lemma: upper bound of expected number of draws},
    \begin{align}
        \mathbb{E}[T_i\left(N\right)] &\leq l' + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l'}^{t-1} \mathbb{P}\left(\hat{m}_{*, s} +  D_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]\right)\nonumber \\ & + \mathbb{P}\left(\hat{m}_{i, s_i} -  D_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}]\right)\\
        & \leq l' + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = 1}^{t-1} 2 t^{-4}\\
        & \leq \frac{ C_{i} \log N}{(\Delta_i L_{i})^2}  + 1 + \frac{\pi^2}{3},
    \end{align}
    where the last step is based on Euler's approach to Basel problem. Then the sum of the expected draws of each sup-optimal draws is
    \begin{align}
        \sd &= \sum_{i: m_i < m_\ast} \mathbb{E}[T_i\left(N\right)] \nonumber \\
        \label{equ: sub-optimal draws bound proof last step}
        &\leq \sum_{i: m_i < m_\ast} \frac{ C_{i} \log N}{(\Delta_i L_{i})^2}  + 1 + \frac{\pi^2}{3},
    \end{align}
where $ C_{i} = 32 \left[(2 + \Delta_i L_{i}) + 2\sqrt{1 + \Delta_i L_{i}}\right]$.

\end{proof}

\subsection{Proof of Corollary \ref{theo: regret bound}}

\begin{proof}
According to the definition of regret shown in Definition \ref{defi: regret for median} and Proposition \ref{prop: expectation of regret and pseudo-regret for median}, we derive the upper bound for expected regret based on Theorem \ref{theo: sub-optimal draws bound},
\begin{align}
    \mathbb{E}[R_N]
    &= \sum_{i = 1}^K \Delta_i \mathbb{E}[T_i\left(N\right)] \nonumber \\
    & \leq
          \sum_{i: m_i < m_\ast}^K \frac{  C_{i} \log N}{\Delta_i L_{i}^2}  + \left(1 + \frac{\pi^2}{3}\right) \left(\sum_{j=1}^K \Delta_{j}\right),
\end{align}
where $ C_{i} = 32 \left[(2 + \Delta_i L_{i}) + 2\sqrt{1 + \Delta_i L_{i}}\right]$.
\end{proof}


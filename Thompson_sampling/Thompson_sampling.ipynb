{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling (Guassian reward with 5-arm bandit simulator)\n",
    "\n",
    "Given the true reward distribution is guassian, we assume the prior distribution is guassian.\n",
    "\n",
    "$$P(\\theta) = \\mathcal{N}(\\mu_0,\\sigma_0^2)$$\n",
    "\n",
    "Based on the Bayesian rule, posterior is\n",
    "\n",
    "$$P(\\theta|r) = \\frac{P(r|\\theta)P(\\theta)}{P(r)} \\propto P(r|\\theta)P(\\theta)$$\n",
    "\n",
    "where $r$ is the reward/observation drawn from the bandit simulator, likelihood $P(r|\\theta) = \\mathcal{N} (r|\\hat{\\theta}, \\sigma^2)$. \n",
    "\n",
    "For each iteration, we sample from the posterior $P(\\theta|r)$, i.e. $\\hat{\\theta} \\sim \\mathcal{N}(\\mu_t, \\sigma_t)$\n",
    "\n",
    "To figure out how to update paremeters of sampler distribution, we need to find the expression of posterior distribution. We the approach 2 mentioned in [Mathematics for Machine Learning Cha9 (p278)](https://mml-book.github.io/book/chapter09.pdf) use the log space, \n",
    "\n",
    "\\begin{align}\n",
    "\\log \\mathcal{N}(r|\\hat{\\theta}, \\sigma^2) + \\log \\mathcal{N}(\\hat{\\theta}|\\mu_t, \\sigma_t^2) \n",
    "& = -\\frac{1}{2} \\{\\sigma ^ {-2} (r - \\hat{\\theta}) ^ 2 + \\sigma_t^{-2} (\\hat{\\theta} - \\mu_t) ^ 2 \\} \\\\\n",
    "& = -\\frac{1}{2} \\{(\\sigma_t^{-2} + \\sigma ^ {-2}) \\hat{\\theta}^2 - 2 (\\sigma_t^{-2} \\mu_t + \\sigma^{-2} r) \\hat{\\theta} \\} + const\\\\\n",
    "\\end{align}\n",
    "\n",
    "That is, we have:\n",
    "$$(\\mu_{t+1},\\sigma_{t+1}) \\gets (\\frac{\\sigma_t^2 r + \\sigma^2 \\mu_t}{\\sigma_t^2 + \\sigma^2}, \\frac{\\sigma_t^2 \\sigma^2}{\\sigma_t^2 + \\sigma^2})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thompson sampling (5-arm bandit with Guassian rewards)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def _ts(num_iter, num_arm):\n",
    "\n",
    "    # initial parameters \n",
    "\n",
    "    update_mu = np.zeros((num_arm)) \n",
    "    update_sigma = np.ones((num_arm)) * 100\n",
    "    liki_sigma = np.ones((num_arm)) \n",
    "\n",
    "    theta_array = np.zeros((num_arm))\n",
    "\n",
    "    for t in range(num_iter):\n",
    "\n",
    "        # sample model\n",
    "        for i in range(num_arm):\n",
    "            theta_array[i] = np.random.normal(update_mu[i], update_sigma[i])\n",
    "\n",
    "        # select and apply action (expection of likilihood is theta)\n",
    "        max_index = np.argmax(theta_array)\n",
    "        #print(max_index)\n",
    "        reward = _simulator(max_index, num_arm)\n",
    "        #print(reward)\n",
    "\n",
    "        # update distribution\n",
    "        tmp_mu = update_mu[max_index]\n",
    "        tmp_sigma = update_sigma[max_index]\n",
    "\n",
    "        update_mu[max_index] = (update_mu[max_index] * liki_sigma[max_index] ** 2.0 + update_sigma[max_index] ** 2.0 * reward)/((update_sigma[max_index] ** 2.0) + (liki_sigma[max_index] ** 2.0))\n",
    "        update_sigma[max_index] = (update_sigma[max_index] ** 2.0) * (liki_sigma[max_index] ** 2.0)/ ((update_sigma[max_index] ** 2.0) + (liki_sigma[max_index] ** 2.0))\n",
    "        #update_mu[max_index] = update_sigma[max_index] * ( update_mu[max_index]/(tmp_sigma ** 2.0) +  reward/(liki_sigma[max_index] ** 2.0))\n",
    "        #print(update_mu[max_index])\n",
    "\n",
    "        '''\n",
    "        if abs(update_mu[max_index] - tmp_mu) < 10 ** (-11) and abs(update_sigma[max_index] - tmp_sigma)< 10 ** (-11):\n",
    "            print(t)\n",
    "            return max_index\n",
    "        '''\n",
    "    return max_index\n",
    "\n",
    "def _simulator(index, num_arm):\n",
    "    '''\n",
    "        simulate five-arm bandit.\n",
    "        input: index (apply x_i arm)\n",
    "        output: reward \n",
    "    '''\n",
    "    # define true distribution of reward for each arm\n",
    "    \n",
    "    #option1\n",
    "    #true_mu = np.array([1,1.1,1.2,1.3,1.4])\n",
    "    #true_sigma = np.ones((num_arm)) * 0.01\n",
    "    \n",
    "    #option2\n",
    "    true_mu = np.array([10.0,20.0,30.0,40.0,50.0])\n",
    "    true_sigma = np.ones((num_arm)) \n",
    "    \n",
    "    reward = np.random.normal(true_mu[index], true_sigma[index])\n",
    "    return reward   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#for i in range(20):\n",
    "print((_ts(500,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, there are some considerations:\n",
    "\n",
    "* How to decide the initial $\\mu_0$ and $\\sigma_0$?  \n",
    "We should pick parameters to span along the $\\theta$ as much as possible, which means a large variance should be picked.\n",
    "* How to choose likelihood $\\sigma$? why we don't need to give a prior on it? \n",
    "* How to decide the true distribution of the badit simulator?   \n",
    "The true distribution should ensure the reward have 'difference' among different arms. By difference, I mean if slightly difference in expectation (e.g. [1,1.1,1.2,1.3,1.4]), then we should pick a small variance (e.g. 0.01)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

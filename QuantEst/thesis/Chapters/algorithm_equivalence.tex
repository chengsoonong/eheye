\chapter{Proof of Algorithm Equivalence}
\label{ch: algo_equal}
% --------------------------------------------------------------------------------
%                                Quantile Estimation
% --------------------------------------------------------------------------------
            
\section{Quantile Estimation}

\subsection{Quantile}

In statistics, quantiles are the points that divide a probability distribution into even intervals.
The $q$-quantiles divide the distribution into $q$ intervals each with the same amount of data points.
And there are $q$ quantile points of the $q-$quantiles.
For example, the $2$-quantile has only one quantile point, which is the middle point of the distribution
and it divides the distribution into two even parts. This $2$-quantile point is called the median.


\subsubsection{Definition} \label{tau-quantile-def}
Generally, the $q$-quantiles have $q-1$ quantile points, and the $k$th $q$-quantile for a 
distribution $X$ is the data value such that

\begin{equation}
    Pr(X \leq x) \geq \frac{k}{q}
\end{equation}

and

\begin{equation}
    Pr(X \geq x) \geq 1 - \frac{k}{q}
\end{equation}

where $x \in X$

\subsection{Quantile Estimation and Pinball Loss}
In this paper, the estimation for $\tau$-quantile 
($\tau =  \frac{1}{q}, \frac{2}{q}, \cdots, \frac{q-1}{q}$)
is applied.
Pinball loss function is one of the approaches for the estimation for a statistical population.
\\\\
For a one-dimensional data set $X = \{x_1, x_2, \cdots, x_N\}$, 
now consider the loss function for a single data point $x$ $(i \in {1, \cdots, N})$.
Let $t := x - q$ be the difference between the real value $x$ and the estimate of quantile $q$.
$l_{\tau}(\cdot): \R \to \R_{\geq 0}$ is the loss function on $t$ such that

\begin{equation}
    l_\tau(t)= 
        \begin{cases}
            \tau t & t > 0\\
            -(1-\tau) t & otherwise
        \end{cases}
\end{equation}


And the $\tau$-quantile loss has the {\color{red} subgradient}:

\begin{equation}
    \frac {\partial l_\tau(t)}{\partial t}= 
        \begin{cases}
            \tau                & t > 0\\
            -(1-\tau)           & t < 0\\
            [\tau, -(1 - \tau)] & t = 0
        \end{cases}
\end{equation}



The overall loss for distribution $X$ with quantile estimation $q$ is

\begin{equation}
    L_{\tau}(q) = \sum_{x \in X} l_{\tau}(x - q)
\end{equation}


The best estimate of the $\tau$-quantile $q$ is the $q$ with minimal overall loss. 
Let $q^\ast$ be the best estimate, then we have
\begin{equation}
    q^\ast = \argmin_{q} L_{\tau}(q)
\end{equation}



% --------------------------------------------------------------------------------
%                                SGD for Quantile Estimation
% --------------------------------------------------------------------------------
\section{Algorithm Equivalence}

\subsection{Pseudo Code for the Frugal-1U Algorithm}

Qiang Ma, S. Muthukrishnan and Mark Sandler {\color{red} [ref]}  
introduced the following algorithm \ref{alg:frugal_1U} which 
"uses only one unit of memory per group to compute a quantile for each group"({\color{blue} quotation}).

\begin{algorithm}
\caption{Frugal-1U}\label{alg:frugal_1U}
    \begin{algorithmic}[1]
        \Require{Data Stream $S$, $h$, $k$, $1$ unit of memory $\tilde{m}$}
        \Ensure{$\tilde{m}$}
        % \Procedure{frugal}{$X,\tau$}            \Comment{X is the dataset}
        \State {Initialization $\tilde{m} = 0$}               %\Comment{Default initialization $q_0$ = 0}
            \For{\textbf{each} $s_i$ in $S$}                  %\Comment{Parameter update for each input data point}
                \State{$rand$ = random(0,1); //get a random value in $[0,1]$}
                % \State {\textbf{set} $\alpha_k$} \Comment{Set stepsize}
                \If{$s_i > \tilde{m}$ \textbf{and} $rand > 1-\frac{h}{k}$} %\Comment{$q_{k+1} = q_k + \alpha_k \tau$ when $x_k - q_k > 0$}
                    \State{$\tilde{m} = \tilde{m} + 1$;}
                \Else { \textbf{if} $s_i < \tilde{m}$ \textbf{and} $rand > \frac{h}{k}$}  %\Comment{$q_{k+1} = q_k - \alpha_k (1-\tau)$ otherwise}
                    \State{$\tilde{m} = \tilde{m} - 1$;}
                \EndIf
            \State{\textbf{end if}}
            \EndFor
        \State{\textbf{end for}}
        % \State \textbf{return} $q$              \Comment{$q_k$ is the SGD result of quantile estimate}
        % \EndProcedure
    \end{algorithmic}
\end{algorithm}
The output $\tilde{m}$ is the estimate of the $h$th $k$-quantile for a given data stream $S$. 
By rephrasing of some steps of Frugal-1U, 
its equilalence to an SGD algorithm for quantile estimation will be shown in the follwing part.
\\\\
\textbf{Rephrasing of the Algorithm} \label{replacements}
\begin{enumerate}
    \item The constant $\frac{h}{k}$ is replaced by $\tau$, since the $\tau$-quantile is defined
     as the $h$th $k$-quantile point in section \ref{tau-quantile-def}.
    % \item The quantile estimate $\tilde{m}$ is replaced by $q$, as it stands for estimate of quantile.
    \item The generation of random number and it's comparison with $1-\frac{h}{k}$ or $\frac{h}{k}$
    in line 3 to 7 is replaced by the following algorithm.
    \begin{algorithm}
        \begin{algorithmic}[1]
            \setcounter{ALG@line}{2}
            \State{ }   \Comment{No need to generate a random number}
            \If{$s_i > \tilde{m}$} %\Comment{$q_{k+1} = q_k + \alpha_k \tau$ when $x_k - q_k > 0$}
                \State{$\tilde{m} = \tilde{m} + 1 \times (1-\frac{h}{k})$}     
                \Comment{$P((rand > 1-\frac{h}{k}) \mid rand \in \mathcal{U}(0,1)) = 1-\frac{h}{k}$;}
            \Else { \textbf{if} $s_i < \tilde{m}$}  %\Comment{$q_{k+1} = q_k - \alpha_k (1-\tau)$ otherwise}
                \State{$\tilde{m} = \tilde{m} - 1 \times \frac{h}{k}$}         
                \Comment{$P((rand > \frac{h}{k}) \mid rand \in \mathcal{U}(0,1)) = \frac{h}{k}$;}
            \EndIf 
        \end{algorithmic}
    \end{algorithm}

    % Here the probability $P((rand > p) \mid rand \in \mathcal{U}(0,1))$ is the simplification
    % for the generation of random number $rand$ and it's comparison to a constant $p$ $(0 < p < 1)$.
    
    To understand this replacement, let's consider the serie of the 3 steps: 
    (i) generate a random number $rand$, 
    (ii) compare it with a constant $p$, and
    (iii) take action if $rand > p$. 
    It can be interpreted as take the action with probability 
    $P((rand > p) \mid rand \in \mathcal{U}(0,1))$. 

    Mathmatically, the replacement works because the expected change of
    $\tilde{m}$ in both methods are the same. 
    For example when $s_i > \tilde{m}$, 
    the expected change of $\tilde{m}$ is
    $E_1[\nabla \tilde{m}] = E[\tilde{m} \times p]$ in the Frugal-1U with 
    random number generation,
    while 
    $E_2[\nabla \tilde{m}] = \tilde{m} \times p$ in the replacement method.
    Since $E_1[\nabla \tilde{m}] = E_2[\nabla \tilde{m}]$, the replacement is valid
    with regard to the expectation of the change in quantile estimate during each step.

\end{enumerate}


\subsection{SGD for Loss function}

Let $q_0$ be the initial guess of quantile estimate. 
By SGD, the estimate is updated each step with a data point from the distribution.
\begin{equation}
    q_{k+1} = q_k - \alpha_k g_k
\end{equation}

where $ \alpha_k $ is a suitable stepsize and 

\begin{equation}
    g_k = \partial L_{\tau}^{(k)}(q_k) \in \frac{\partial l_\tau(x_k - q_k)}{\partial q_k}
\end{equation}
 
\textbf{Notice: partial is taken because the gradient of a single variable function
euqals the partial of it}
\\\\
Then we have
\begin{equation*}
    q_{k+1} = 
    \begin{cases}
        q_k + \alpha_k \tau               & x_k - q_k > 0\\
        q_k - \alpha_k (1-\tau)           & x_k - q_k \leq 0\\
        % [\tau, -(1 - \tau)] & t = 0
    \end{cases}
\end{equation*}


\begin{algorithm}
    \caption{SGD algorithm}\label{alg:SGD}
    \begin{algorithmic}[1]
        \Require{Data Stream $X$, $\tau$, $1$ unit of memory $q$}
        \Ensure{$q$}
        % \Procedure{frugal}{$X,\tau$}            \Comment{X is the dataset}
        \State {Initialize} $q$                 \Comment{Default initialization $q_0$ = 0}
            \For{$x_k$ in $X$}                  \Comment{Parameter update for each input data point}
                \State \textbf{set} $\alpha_k$  \Comment{Set stepsize}
                \If{$x_k > q$}                  \Comment{$q_{k+1} = q_k + \alpha_k \tau$ when $x_k - q_k > 0$}
                    \State{$q = q + \alpha_k \tau$}
                \Else                           \Comment{$q_{k+1} = q_k - \alpha_k (1-\tau)$ otherwise}
                    \State{$q = q - \alpha_k (1-\tau)$}
                \EndIf
            \EndFor
        \State \textbf{return} $q$              \Comment{$q_k$ is the SGD result of quantile estimate}
        % \EndProcedure
    \end{algorithmic}
\end{algorithm}
% --------------------------------------------------------------------------------
%                              Equality of two algorithms 
% --------------------------------------------------------------------------------
\section{Equivalence of Algorithms}
In this section we'll show the equilalence of algorithm Frugal-1U 
and SGD.
\\\\
Besides the replacements mentioned in section \ref{replacements},
the notations have changed: 
$X$ is applied for the data stream, and $q$ instead of $\tilde{m}$
to represent quantile estimate.
The introduction of changable stepsize $\alpha_k$ for each data point $x_k$
is the highlight of the SGD algorithm. The flexibility of stepsize can help
with achieving a better convergence rate when stepsizes are chosen wisely.
Specifically, the setpsize is not mentioned in Frugal-1U 
because it is fixed as $1$. In SGD the stepsize might change for every step.

\marginpar{
{\color{red} \textbf{Problem}}
In Frugal-1U the quantile estimate $q$ does not change when $x_k > q$, but in SGD 
the quantile estimate is updated: $q = q-\alpha_k (1-\tau)$. This can be seen as different
} 

\textbf{subgradient} values for $l_\tau(x_k - q_k)$ 
{\color{red} \textbf{but then I need to explain subgradient descent}} 


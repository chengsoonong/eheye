\chapter{Conclusion}
\label{ch: conclusion}

% Problem 
% Why difficult
% Previous work
% My work
% Why important

This thesis focuses on the problem of space efficient and computationally efficient quantile estimation on data streams.
Determining quantiles helps characterize a data distribution, however due to the nature of data streams, it faces challenges in both storage and computation. The calculation of batch quantiles becomes infeasible due large amount of data, coming in at a high rate.
To solve this issue, algorithms of quantile estimation on data streams have been proposed which aim to keep a slow growing memory usage and low computational requirements for relatively accurate quantile estimates.
In this paper, we investigate the implementation of SGD method which uses constant memory and computation time per iteration for data streams of any size.
% To answer our research question, the SGD approach can be effectively applied for quantile estimation.
\\\\
We derive the SGD algorithm by implementing the SGD approach for quantile estimation. The proposed SGD algorithm is equivalent, both theoretically and empirically, to the Frugal-1U algorithm to some extent. Precisely, the algorithm equivalence analysis illustrates how the highly similar behaviours and results are affected by their difference in randomness. Since the evaluation results satisfy our tolerant requirements for equivalence, we conclude that the two algorithms are equal. This also implies that the SGD is a valid method for quantile estimation.

The convergence of SGD was empirically found to depend on several factors,
namely data distribution, data size and SGD step size settings. Data ordering was not found to have a significant impact on convergence. Of all applications, the only factor under the control of the user is the SGD setting step size. 

The newly proposed adaptation approaches of SGD step size, SAG and DH-SGD, are found to have improvement effects on convergence rate. Specifically, the SAG is theoretically proved to have a faster convergence rate while the DH-SGD convergence is tested only empirically. 
We discussed 2 smooth loss function approximations for SAG, but neither completely resolves the issue of convexity and continuity.
% Explore the extension on multi-quantile estimation. Compared two algorithms shiftQ and P2.

\section{Future work}

Step size is not the only SGD setting that affects the quantile estimation performance. As is mentioned before, other SGD settings like initialization of quantile estimates are also potentially influential factors. In the current setting of SGD, the initialization of any $\tau$-quantile ($\tau \in (0,1)$), the initialization is always $0$. The study on estimate initialization methods, for example using the first few sorted observations as starting points, is a possible direction for future work.

Another research direction is to explore more on our two current step size adaptation algorithms. Both SAG and DH-SGD are shown to have empirical acceleration effects on convergence, but they both have the unstable fluctuation problem after convergence as well as other limitation. Especially, we are concerned about the lack of convergence guarantee from DH-SGD although it provides significant improvements in some situations. It is worthwhile to consider a new algorithm that combine the idea of both SAG and DH-SGD in step size adaptation such that the convergence is guaranteed and the step size is improved.

In addition to step size adaptation, another alternative improvement direction called simultaneous multi-quantile estimation is illustrated in chapter~\ref{ch: multi_quant}. 
The shiftQ and $P^2$ algorithms introduce different ways of keeping positive distance between each adjacent quantiles at each update iteration. It is possible that some future SGD method can implement their ideas to guarantee the monotone property of a sequence of quantile estimates.

One of the ignored issue is the use of gradient descent on the non-smooth function. Theoretically, gradient descent is not applicable for the pinball loss function which is not differentiable at $0$, this application of SGD is in theory not entirely valid. However, in practice the possibility of reaching the $0$ point is $0$, and the setting of the gradient to zero at it makes sense. It is also shown in the experiment that the performance of the SGD algorithm works effectively in spite of the defective algorithm deduction. In theory, however, there is still work for the validness of SGD.


\section{Summary}
Stochastic gradient descent is a workhorse of modern machine learning methods. Here it is used to analyse quantile estimation from data streams, and is shown equivalent to a recent method from the database systems literature. We explore the empirical performance of SGD, step size adaptation approaches and two algorithms for estimation of multiple quantiles. We hope that this connection between a machine learning approach and a streaming data problem in data analysis will provide a fruitful avenue of future research.
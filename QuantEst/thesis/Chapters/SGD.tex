% \documentclass[11pt]{article}

\chapter{Stochastic Gradient Descent}
\label{ch: sgd}
% \begin{document}
% \maketitle

\section{Stochastic Gradient Descent}

    Stochastic gradient descent (often abbreviated SGD) is an optimization algorithm developed from gradient descent. 
    In this section, gradient descent is introduced as the first part of the explanation of SGD.

    \subsection{Gradient Descent}
        For convex optimization problems, gradient descent is a first-order optimization algorithm 
        to find the local minimum of a function.
        \\\\
        To solve the minimization problem 
        \begin{equation}
            % E
            \min_{\x} L(\x) 
        \end{equation} 
        
        where $L : \R^d \to \R$ is convex, differentiable and its gradient is Lipschitz continuous with constant
        $L > 0$.
        \\\\
        Geometrically, the gradient $\nabla L(\x_0)$ points to the direction of the steepest ascent on $L(\cdot)$ 
        from the point $\x_0$. 
        By taking a small step in the direction of the negative gradient, the function value is decreased in the 
        direction of the steepest descent. That is,
        \begin{equation}
            \x_1  = \x_0 - \alpha \nabla L(\x_0)
        \end{equation}
    
        for a small enough stepsize $\alpha \in \R_{+}$, then $L(\x_1) \leq L(\x_0)$. 
        That means, compared with $L(\x_0)$, $L(\x_1)$ is closer to the local minimum.
        \\\\
        With this observation comes the idea of gradient descent: an iterative "tour" on $L(\cdot)$ from a point towards the 
        local minimum by following small steps of negative gradient. 
        Let $\x_0$ be the guess of a starting point, then if
        \begin{equation}
            \x_{k+1} = \x_{k} - \alpha_k \nabla L(\x_k), k \geq 0
        \end{equation}
        
        
        Then we have $ L(\x_0) \geq L(\x_1) \geq L(\x_2) \geq \cdots$ with suitable $\alpha_k$. The convergence of the 
        sequence $(\x_n)$ to the local minimum is guaranteed{\color{red} [reference]}.


    \subsection{Stochastic Gradient Descent}
        SGD can be considered as a stohcastic appoximation of gradient descent optimization, 
        when the objective function $L(\cdot)$ can be written as a sum of differentiable functions.
        Consider the objective function is in the form:
        \begin{equation}
            L(\x) =\frac{1}{K} \sum_{k=1}^{K} L_k (\x)
        \end{equation}

        where the summand function $L_k$ is usually the loss function of the $k$th observation among
        $K$ data points.
        \\\\
        Then by following the idea of gradient descent, the $\x$ is updated according to
        
        \begin{equation}
           \x_{k+1} = \x_{k} -\alpha_k \nabla L(\x_k) = \x_{k} -\alpha_k \frac{1}{K}\sum_{k=1}^{K} \nabla L_k(\x_k) 
        \end{equation}
        
        
        where each $\alpha_k$ is a suitable stepsize. The calculation of $\sum_{k=1}^{K} \nabla L_k(\x_k)$ can be
        expensive, especially when the amount of summand functions is huge, or when the individual gradients are hard to
        compute. 
        \\\\
        To reduce the consumption of calculation, an estimation of the true gradient of $L(\x)$ is taken: 
        the true gradient $\frac{1}{K} \sum_{k=1}^{K} \nabla L_k(\x_k)$ is replaced by the gradient of a single observation $\nabla L_k(\x_k)$. 
        So the update of the parameter $\x$ becomes
        
        \begin{equation}
            \x_{k+1} = \x_{k} - \alpha_k \nabla L_k(\x_k)
        \end{equation}
        
        where $\alpha_k$ is a suitable stepsize. 
        \\\\
        The convergence of SGD has been proved as well{\color{red} [reference]}. 
        \\\\\\
        (and should I explain more about why stochastic gradient descent works?)

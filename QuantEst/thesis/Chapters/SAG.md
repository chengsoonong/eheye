
SAG

1. Why SAG:
   1. linear **convergence** rate, small variance.
   2. Applies for really big N, convex optimization, lipschitz continuous 
2. Explanation & detail:
   1. the equation
   2. **How to deal with non-smoothness for pinball loss?** -> practically zero possibility to reach the point 0 + can use smooth function
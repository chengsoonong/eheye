% \documentclass[12pt]{article}
% \usepackage{xcolor}
% \input{nams.tex}
% \usepackage[numbers]{natbib}

% \title{Literature review}
% \date{\vspace{-5ex}}


% \begin{document}
% \maketitle
\chapter{Literature review}
\label{ch: literature_review}


The field of quantile estimation on data streams is well studied, and this thesis is incapable of covering all the topics.
This chapter details some of the work that has been done in this field, and is organised as follows:

Section~\ref{streamingdata} introduces the data stream phenomenon and some quantile estimation methods on it, classified by algorithm type. We specially highlight the space complexity of those algorithms, to show the improvements of quantile estimation algorithms in an intuitive way.

Section~\ref{singlequantile} focuses on algorithms with extreme restriction: estimate a quantile in constant space complexity. Two methods are described in detail, followed by a brief comparison.

Since algorithms in section~\ref{singlequantile} cannot deal with multiple quantiles, section~\ref{multiquantile} introduces an improved constant space usage algorithm for quantile summary queries, which is the inspiration of our work.
%----------------------------------------------------------------------------------------
%   Quantile estimation methods on streaming data
%----------------------------------------------------------------------------------------

\section{Quantile estimation methods on streaming data}
\label{streamingdata}
A data stream is the phenomenon that input data comes at a high rate instead of being accessible all at the same time\cite{muthukrishnanDataStreamsAlgorithms2005}.
Generally speaking, the continuous data can come without a start or end, or consistent time intervals.
Streaming data has become popular in industrial application, especially big data use cases, for it
``includes a wide variety of data such as log files generated by customers using your mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.''\cite{WhatStreamingData}.

Compared with instantly accessible data, three challenges stand out in relation to data streams: transmission, computation and storage\cite{muthukrishnanDataStreamsAlgorithms2005}.
Transmission is difficult when there are frequent delays or the data is in a complicated format,
while challenges in computation stem from the need for algorithms to keep up with high rate data streams.
Finally, it is infeasible to store all the data produced by large data streams.
In this research, we focus only on the latter two problems. We limit our scope to programs with low computational complexity and restricted memory use, under the assumptions that the input data is low-dimensional and is not transmitted at a high rate.
\\\\
We assume that the data points produced by a data stream are independent identically distributed samples from an unknown distribution. 
Although access to the distribution is impossible, analysis of a random sample can reveal some useful information. 
A quantile is an important feature of a distribution, which represents the cutting points which divides a data distribution by its ranking.
The computation of a quantile is relatively easy as long as the dataset is sorted.
A data stream, however, is updated at the arrival of each single input, hence quantile calculation demands that the data is sorted again every update.
What's worse, the unknown number of data points can cause a severe memory shortage problem for any system. 
The conclusion now becomes clear that quantile calculation fails to be a feasible solution for data streams.

Quantile estimation replaces calculation for data streams due to the computation and storage problems.
The estimation works with limited computational power and memory storage.
The batch algorithm for quantile computation takes $O(N)$ space.
Generally, quantile estimation algorithms trade off accuracy for computation and storage.
\citeauthor{munroSelectionSortingLimited1980} \cite{munroSelectionSortingLimited1980} have shown that any algorithm that computes the exact $\tau$-quantile from an $N$-element data stream with $p$ passes requires at least $\Omega(N^{\frac{1}{p}})$ space.
Therefore quantile estimation algorithms all necessarily aim for sub-linear space.
In this chapter, we investigate the accuracy and the memory usage of a variety of existing algorithms for quantile estimation. Section~\ref{deterministic},~\ref{randomised} and~\ref{other} divide the algorithms by their types: deterministic, randomised and others respectively. The classification is based on works from \citeauthor{buragohainQuantilesStreams2009}\cite{buragohainQuantilesStreams2009} and \citeauthor{wangQuantilesDataStreams2013}\cite{wangQuantilesDataStreams2013}.
% Model: time series: each input $a_i$ equals $A[i]$ and appears in increasing order.
% One-pass application: the reading of an input is done in exactly once and all inputs are read in order. Usually, the process for each input takes only one or few passes. 

It is worth noting that since quantiles are related with ranking, a quantile estimation algorithm takes one-dimensional numerical data input by default. The simple data format also partially explains why transmission is not usually considered a problem in most quantile estimation algorithms.
% -----------------------------------------------------------------------------
% to be finished
\subsection{Deterministic algorithms}
\label{deterministic}
% Manku's work
% \marginpar{{I dont know why the \citeauthor{mankuApproximateMediansOther1998}\cite{mankuApproximateMediansOther1998} method is deterministic. Also confused about the Monte Carlo randomization mentioned in \citeauthor{wangQuantilesDataStreams2013}\cite{wangQuantilesDataStreams2013}}}
% \textbf{A deterministic algorithm for quantile estimation is (to be finished).}
The first deterministic algorithm on quantile estimation was proposed by \citeauthor{mankuApproximateMediansOther1998}\cite{mankuApproximateMediansOther1998} (referred to as the MRL algorithm), based on the work by \citeauthor{munroSelectionSortingLimited1980}\cite{munroSelectionSortingLimited1980}. Along with the algorithm, the notation \textit{$\epsilon$-approximate $\tau$-quantile} is first introduced for accuracy measurement by \citeauthor{mankuApproximateMediansOther1998}, which describes the property of guaranteed accuracy on $\tau$-quantile within a pre-specified precision $\epsilon$. The algorithm has a space complexity $O(\frac{1}{\epsilon}\log^2 (\epsilon N))$ to compute an \textit{$\epsilon$-approximate quantile summary}. Building on this, two improved algorithms have been proposed by \citeauthor{greenwaldQuantilesEquidepthHistograms2016a}\cite{greenwaldQuantilesEquidepthHistograms2016a} and \citeauthor{shrivastavaMediansNewAggregation2004b}\cite{shrivastavaMediansNewAggregation2004b}.

\citeauthor{greenwaldQuantilesEquidepthHistograms2016a}\cite{greenwaldQuantilesEquidepthHistograms2016a} propose an algorithm (referred to as the GK algorithm) that has a worst-case space requirement of $O(\frac{1}{\epsilon}\log(\epsilon N))$, surpassing the MRL algorithm both theoretically and empirically. The general idea of GK algorithm maintains a sorted subset of data input by the combine and prune operations which keep the precision in control. The space complexity was then improved to 
$O(\frac{1}{\epsilon}\log^{\frac{3}{2}}\frac{1}{\epsilon})$ by \citeauthor{agarwalMergeableSummaries2013}\cite{agarwalMergeableSummaries2013}, who provide a mergeable algorithm based on the GK algorithm.

A deterministic, fixed-universe algorithm \textit{Q-Digest} was designed by \citeauthor{shrivastavaMediansNewAggregation2004b}\cite{shrivastavaMediansNewAggregation2004b}. It deals with the quantile problem as a histogram problem using $O (\frac{1}{\epsilon}\log U)$ space, where $U$ is the size of the fixed universe from which the input is drawn.


\subsection{Randomised algorithms}
\label{randomised}

Estimating quantile from a random sample of data stream input is the basic idea for most randomized algorithms, and it reduces space usage to a considerable extent.
A simple randomized algorithm designed by \citeauthor{floydExpectedTimeBounds1975}\cite{floydExpectedTimeBounds1975} uses $O(\frac{1}{\epsilon^2} \log \frac{1}{\epsilon \delta})$ space, for an $\epsilon$-approximate $\tau$-quantile of N elements with probability at least $1 - \delta$, where $0 < \delta < 1$.
Later, \citeauthor{mankuRandomSamplingTechniques1999}\cite{mankuRandomSamplingTechniques1999} proposed a randomized quantile estimation algorithm based on their previous framework in \cite{mankuApproximateMediansOther1998}. The new method requires 
$O(
    \frac{1}{\epsilon} \log^2 
    (\frac{1}{\epsilon} 
        \log \frac{1}{\epsilon \delta}
    )
)
$
space, being the first practical algorithm that does not require advance knowledge of data stream size $N$.

\textit{Turnstile models} accepts both addition and deletion of data points from a data stream.
The turnstile model was first introduced by \citeauthor{gilbertChapter40How2002}\cite{gilbertChapter40How2002}, who designed the \textit{random subset sum} quantile estimation algorithm with space complexity 
$O(\frac{1}{\epsilon^2} \log^2 U \log (\frac{\log U}{\epsilon}))$, where $U$ is the size of the universe. 
The key idea is based on their findings that insertions or deletions of quantiles can be reduced to finding range sums.
The space requirement is greatly improved by another algorithm for turnstile model -- the \textit{Count-Min sketch} designed by \citeauthor{cormodeImprovedDataStream2005}\cite{cormodeImprovedDataStream2005}. Only 
$O(\frac{1}{\epsilon} \log^2 N \log (\frac{\log N}{\tau \delta}))$ space is needed for an $\epsilon$-approximate $\tau$-quantile. Although the space requirement is worse than the deterministic algorithms \textit{GK} and \textit{Q-Digest}, it has a noticeable advantage on dealing with both insertions and deletions of data to streams.

Another significant improvement on randomised algorithms were made in 2017, when \citeauthor{felberRandomizedOnlineQuantile2017}\cite{felberRandomizedOnlineQuantile2017} developed an algorithm that uses $O(\frac{1}{\epsilon}\log \frac{1}{\epsilon})$ space. The algorithm not only matches the previous best upper bound of $O(\frac{1}{\epsilon}\log^{\frac{3}{2}}\frac{1}{\epsilon})$ by \citeauthor{agarwalMergeableSummaries2013}\cite{agarwalMergeableSummaries2013}, but it also beats the lower bound for any deterministic comparison-based algorithm, according to the work of \citeauthor{hungLogSpaceLower2010}\cite{hungLogSpaceLower2010}. The estimation ensures the return of the $\epsilon$-approximate quantile summary, with probability at least $1-e^{-poly(1/\epsilon)}$.


\subsection{Algorithms for other requirements}
\label{other}
Due to the large range of data streams and quantile application requirements, it is not surprising to have a considerable variety in quantile estimation algorithms attacking specific requirements. We will introduce some interesting applications in the following, such as the methods for biased accuracy requirements and a parallel algorithm for distributed data streams.
\\\\
Although most quantile estimation work targets the \textit{uniform accuracy} problem on quantile estimation, many queries over data streams require different accuracy for different quantiles. For example, the $0.99$-quantile is usually sparsely surrounded while the $0.5$-quantile is likely to locate in the center of a distribution where the data is much more dense. Thus comes the \textit{biased quantiles} problem which demands higher accuracy for more extreme quantile values.
The first deterministic algorithms for the biased quantiles problem was proposed by \citeauthor{cormodeSpaceTimeefficientDeterministic2006} \cite{cormodeSpaceTimeefficientDeterministic2006} and takes only $O(\frac{1}{\epsilon} \log {U} \cdot \log {\epsilon N})$ space, where $N$ is the size of the data stream and $U$ is the size of universe from which the samples are drawn. Besides, the algorithms also apply to other related problems like biased rank queries and targeted quantiles. 

There are also applications that require algorithms to work for distributed systems where a big data stream is processed by different processors.
\citeauthor{ben-haimStreamingParallelDecision2010}\cite{ben-haimStreamingParallelDecision2010} introduce an on-line histogram building algorithm \textit{Streaming Parallel Decision Tree (SPDT)} in which the histogram boundaries can be seen as estimated quantile values.
In this method, multiple histograms are built from streaming data in parallel, which are then merged into a summary histogram of the entire dataset. The summary histogram is a set of sorted real numbers that represents the interval boundaries such that all the intervals contain approximately the same amount of data. Specifically, for a summary histogram with $K$ intervals, the set of real numbers is approximately the set of $\tau$-quantiles ($\tau = \frac{1}{K}, \frac{2}{K}, ..., \frac{K-1}{K}$) for the input data stream.
Some quantiles estimation methods mentioned before are also available for distributed settings. To mention a few,
{Q-Digest}\cite{shrivastavaMediansNewAggregation2004b} is a tree based algorithm which can be extended for distributed quantile processors by merging the quantile summaries across different nodes. The Count-Min sketch\cite{cormodeImprovedDataStream2005} is also suited for distributed applications too. In both cases, parallelisation does not change the space complexity.
The {GK} algorithm also has a structure that maintains distributed nodes, and \citeauthor{greenwaldPowerconservingComputationOrderstatistics2004}\cite{greenwaldPowerconservingComputationOrderstatistics2004} found a method to fit it in distributed settings with a larger space complexity $O(\frac{1}{\epsilon}log^3 N)$.
It was \citeauthor{agarwalMergeableSummaries2013}\cite{agarwalMergeableSummaries2013} who made the significant improvement to lower the space complexity to 
$O(\frac{1}{\epsilon}\log^{\frac{3}{2}}\frac{1}{\epsilon})$
with the capability to work in a distributed setting.

In the situation when old data expires, it is desirable to have quantiles of only the most recent observations from a data stream.
The \textit{sliding window} represents the most recent $W$ observations in a data stream. The types of sliding window can be classified as either \textit{fixed-size} or \textit{variable-size}.
In \cite{linContinuouslyMaintainingQuantile2004}, \citeauthor{linContinuouslyMaintainingQuantile2004} posed their algorithm for the fixed-size sliding window problem with an 
$O(\frac{1}{\epsilon} \log (\epsilon^2 W) + \frac{1}{\epsilon^2})$ space requirement.
The result was then beaten by the work of \citeauthor{arasuApproximateCountsQuantiles2004}\cite{arasuApproximateCountsQuantiles2004} with space complexity 
$O(\frac{1}{\epsilon} \log\frac{1}{\epsilon} \log W)$,
which are designed for both fixed and variable sized windows.
Despite of a worse memory space usage, the work of \citeauthor{linContinuouslyMaintainingQuantile2004}\cite{linContinuouslyMaintainingQuantile2004} has the advantage of estimating quantiles for the most $w$ observations ($w < W$) with $O(\frac{1}{\epsilon^2} \log (\epsilon W))$ space requirement.


    % This summary histogram is notable for its evenly distributed intervals sizes, as each interval has the same number of data points. To interpret the histogram into quantiles, 

% The stochastic gradient descent algorithm, however, is proposed under the assumption that the si` zze of data stream samples is unknown. \textbf{i dont know what to write for the accuracy/convergence part QAQ}
%----------------------------------------------------------------------------------------
%   Single quantile estimation
%----------------------------------------------------------------------------------------

\section{Non-growing space for single quantile estimation}
\label{singlequantile}
The brief history above has shown a clear path for quantile estimation algorithm development: keeping a relatively accurate result while using as little memory as possible. This triggers another interesting question: what if only a constant memory is allowed, how would it affect the accuracy on quantile estimation? In the following part, we introduce two similar algorithms that estimate a single quantile with a $O(1)$ space requirement.
% \textbf{Online algorithms: Data stream algorithms have an online component where input is revealed in steps, but they have resource constraints that are not typically incorporated in competitive analysis of online algorithms. \cite{muthukrishnanDataStreamsAlgorithms2005}}
\\\\
\citeauthor{maFrugalStreamingEstimating2014}\cite{maFrugalStreamingEstimating2014} introduce the randomized algorithms frugal streaming (referred to as Frugal) that require only one or two units of memory. This algorithm does not need prior knowledge of the size of input data or the universe, and the query of quantile estimation can be made after the arrival of any item.
It is worth noticing that Frugal is significantly different from the randomized algorithms introduced before.
As mentioned before, the key idea of randomized algorithms is the maintenance of a random set of samplings from the data stream. Frugal, however, is not able to keep such a samplings with its extremely limited space, its randomness is caused by another design of the algorithm.

The \textit{Frugal-1U} algorithm stores only the current quantile estimate, and the estimate is updated with randomness on the arrival of each new observation. As the authors describe, the current estimate ``drifts'' in the direction indicated by each new item\cite{maFrugalStreamingEstimating2014}. That is, when the current estimate equals the new observation, the value is not changed. When the new observation is greater than current estimate, then the estimate is increased with some probability; otherwise it decreased with some probability.
The pseudo code of the algorithm is shown below.

\begin{algorithm}
    \caption{Frugal-1U}\label{alg:frugal_1U}
        \begin{algorithmic}[1]
            \Require{Data Stream $S$, $\tau$, $1$ unit of memory $\tilde{q}$}
            \Ensure{$\tilde{q}$}
            % \Procedure{frugal}{$X,\tau$}            \Comment{X is the dataset}
            \State {Initialization $\tilde{q} = 0$}               %\Comment{Default initialization $q_0$ = 0}
                \For{\textbf{each} $s_i$ in $S$}                  %\Comment{Parameter update for each input data point}
                    \State{$rand$ = random(0,1); //get a random value in $[0,1]$}
                    % \State {\textbf{set} $\alpha_k$} \Comment{Set stepsize}
                    \If{$s_i > \tilde{q}$ \textbf{and} $rand > 1-\tau$} %\Comment{$q_{k+1} = q_k + \alpha_k \tau$ when $x_k - q_k > 0$}
                        \State{$\tilde{q} = \tilde{q} + 1$;}
                    \Else { \textbf{if} $s_i < \tilde{q}$ \textbf{and} $rand > \tau$}  %\Comment{$q_{k+1} = q_k - \alpha_k (1-\tau)$ otherwise}
                        \State{$\tilde{q} = \tilde{q} - 1$;}
                    \EndIf
                \State{\textbf{end if}}
                \EndFor
            \State{\textbf{end for}}
            % \State \textbf{return} $q$              \Comment{$q_k$ is the SGD result of quantile estimate}
            % \EndProcedure
        \end{algorithmic}
    \end{algorithm}
Specifically, the output $\tilde{q}$ is the estimate of the $\tau$-quantile for a given data stream $S$.
In line 3\textasciitilde 8 of the pseudo code, the algorithm get randomness in its estimate update procedure, for the update is done by probability compared with $1- \tau$ or $\tau$.
% An improved version is Frugal-2U, which needs another memory unit for a better convergence rate. 
\\\\
To attack the same problem, \citeauthor{yazidiQuantileEstimationDynamic2016}\cite{yazidiQuantileEstimationDynamic2016} resort to the idea of online learning and achieves a similar method. 
One distinction is step size. In this algorithm, each step the new estimate is updated by a variable related with the current estimate rather than a constant.
Their work is inspired by \citeauthor{tierneySpaceEfficientRecursiveProcedure1983}\cite{tierneySpaceEfficientRecursiveProcedure1983}, who introduce stochastic learning method to quantile estimation problems. 
% In The noticeable similarity and difference between \citeauthor{yazidiQuantileEstimationDynamic2016}'s work and the Frugal-1U is analysed in \textbf{section ??}.  


%----------------------------------------------------------------------------------------
%   Multi- quantile estimation
%----------------------------------------------------------------------------------------

\section{Non-growing space for multi-quantile estimation}
\label{multiquantile}
One new problem for the constant space usage algorithms is the limit in number of quantiles estimated for each query.
When more quantile numbers are required, for example, two quantiles each identifying the upper and lower outlier of a distribution, the single quantile estimation methods become inefficient and less applicable.
For only a few quantiles, this problem can be solved by running parallel single quantile estimation processes for each quantile. But the issue remains when the number of processes exceeds the computer's parallel capacity.
This suggests the need for an algorithm which estimates several quantile values in one process, as a general solution to this problem.
Multiple quantile estimation, or quantile summary estimation, is the simultaneous estimation of different quantile values from streaming data.
In general, it estimates $K$ quantiles (the $\tau_1$-, $\tau_2$-, $...$, $\tau_K$-quantiles) at the same time.
This is a problem targeted by several different algorithms.

% \marginpar{Want to include the work of \citeauthor{hammerJointTrackingMultiple2019b}\cite{hammerJointTrackingMultiple2019b}: Multiplicative Update Methods for Incremental Quantile Estimation}

\textit{$P^2$} is another multi-quantile estimation method proposed by \citeauthor{jainP2AlgorithmDynamic1985}\cite{jainP2AlgorithmDynamic1985}. It assumes a pairwise-parabolic relationship between every three adjacent quantiles, and update quantile estimates based on their relative positions with the neighbour quantiles. Later, an extension version \textit{Extended $P^2$} was developed by \citeauthor{raatikainenSequentialProcedureSimultaneous1993}\cite{raatikainenSequentialProcedureSimultaneous1993}, which changed the initialization of the algorithm to improve the accuracy.


\subsection{Other methods and advantages of simultaneous estimation}
% \marginpar{Not sure if I should keep this part}

Another single-pass low-memory method for simultaneous multi-quantile estimation is the Data Skeleton(DS)\cite{mcdermottDataSkeletonsSimultaneous2007} algorithm, which is derived from the method proposed by \citeauthor{liechtySinglepassLowstorageArbitrary} \cite{liechtySinglepassLowstorageArbitrary}. For an estimation of $k$ quantiles, the algorithm requires the first $km$ data points($m$ is a constant) being sorted, and updates this tracking array on each new observation. Instead of the $k$ estimates of quantiles, it returns a total of $km$ estimates due to the redundancy of computation. This feature is considered an advantage for certain applications like density estimation, when extra quantile estimates is useful in accuracy improvement.


Over the comparison with \citeauthor{liechtySinglepassLowstorageArbitrary}'s algorithm, \citeauthor{mcdermottDataSkeletonsSimultaneous2007}\cite{mcdermottDataSkeletonsSimultaneous2007} find simultaneous estimation on multiple quantiles has two main advantages over single quantile estimation methods. First is the reduction in computation time as it does not need to estimate quantiles separately. The second advantage, according to the experiments, is the accuracy improvement in simultaneous quantile estimation.  


% \section{Anomaly Detection and Outlier}

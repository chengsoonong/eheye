\documentclass[12pt]{article}
\usepackage{xcolor}
\input{nams.tex}
\usepackage[numbers]{natbib}

\title{Literature review}
\date{\vspace{-5ex}}


\begin{document}
\maketitle

The field of quantile estimation is big, and ??
This chapter details some of the work that has been done in the quantile estimation on streaming data, organised as follows:

Section~\ref{streamingdata} lists some current quantile estimation methods on the streaming data phenomenon, and classifies them by algorithm types. 
% while section~\ref{singlequantile} and ~\ref{multiquantile} classify quantile estimation problems by aims

Section~\ref{singlequantile} discusses algorithms on single quantile estimation, and especially details two methods for restricted memory storage limit.

Section~\ref{multiquantile} presents algorithms on the problem of multi-quantile estimation, which estimate multiple quantiles at the same time, and the methods are 

   \section{Quantile estimation methods on streaming data}
   \label{streamingdata}
   Data stream is the phenomenon that input data comes at a high rate instead of being accessible at the same time. Compared with instantly accessible data, three challenges stand out their potentially higher difficulties: transmission, computation and storage\cite{muthukrishnanDataStreamsAlgorithms2005}. 
   In this research, we focus on programs with low computation complexity and restricted memory storage, under the assumption that the low-dimensional input data is transmitted at a not very high rate.
   
   Quantile estimation on streaming data aims at an accurate to the 
   Model: time series: each input $a_i$ equales $A[i]$ and appears in increasing order.
   One-pass application: the reading of an input is done in exactly once and all inputs are read in order. Usually, the process for each input takes only one or few passes. 
%    In contrast to some phenomena-driven data streams, 

   The frequently used ``one pass" quantile estimation model is well-studied with regards to its effectiveness and limitations.
    \\\\
    Classification based on \cite{buragohainQuantilesStreams2009}

    \subsection{Deterministic algorithms}
    \label{deterministic}
    \textbf{RE-WRITE } 

    \citeauthor{greenwaldQuantilesEquidepthHistograms2016a}\cite{greenwaldQuantilesEquidepthHistograms2016a} analyse specific algorithms on streaming data with regards to some quantile estimation properties. 
    They use the measure \textit{$\epsilon$-approximate $\phi$-quantile} to describe the property of guaranteed accuracy on $\phi$-quantile within a pre-specified precision $\epsilon$. 
    For simultaneous estimation on multiple quantiles, a \textit{$\epsilon$-approximate quantile summary} is defined to be the set of multiple ordered $\epsilon$-approximate quantiles.
    The algorithm analysis focuses on deterministic algorithms that satisfies the MRL framework based on the work of \citeauthor{mankuApproximateMediansOthera} \cite{mankuApproximateMediansOthera}. To compute an $\epsilon$-approximate quantile summary, it is shown that the currently best known algorithms needs $O(\frac{log(\epsilon N)} {\epsilon})$ space.

    \textbf{ this is copied from others!\\
        \citeauthor{shrivastavaMediansNewAggregation2004} \cite{shrivastavaMediansNewAggregation2004}
         presented another deterministic algorithm for insert-only streams that uses (?? log U) space, where U is the size of the domain from which the input is drawn
        }


    \subsection{Randomised algorithms}
    \label{randomised}
    \cite{guhaStreamOrderOrder2009}


    \subsection{Quantile estimation under other requirements}
    \label{other}
    For specific estimation accuracy requirements, space and time limitations vary. For example, the biased quantiles problem requires higher accuracy for more extreme quantile values. \citeauthor{cormodeSpaceTimeefficientDeterministic2006} \cite{cormodeSpaceTimeefficientDeterministic2006} propose a deterministic algorithm that takes only space $O(\frac{\log {U}}{\epsilon} \log {\epsilon N})$ for a biased quantiles with $\epsilon$ approximation ($U$ is the size of universe from which the samples are drawn). 
    Another application aims to attack on the multi-thread data stream problem. \cite{ben-haimStreamingParallelDecision} 

% The stochastic gradient descent algorithm, however, is proposed under the assumption that the si` zze of data stream samples is unknown. \textbf{i dont know what to write for the accuracy/convergence part QAQ}

    \pagebreak
    \section{Single quantile estimation with non-growing memory}
    \label{singlequantile}
    Memory storage restrictions from data stream can be stronger for some applications, which asks for algorithms with memory space not affected by sample size. 
    \textbf{examples/explanations}
    Several algorithms have been proposed to deal with this challenges, and two of them are discussed in detail later in this research.
    
    
    \textbf{
    \citeauthor{mankuApproximateMediansOthera} \cite{mankuApproximateMediansOthera} uses $bk$ memories, where $b$ is the number of buffer that each can store $k$ elements.
    \\
    \citeauthor{dunningComputingExtremelyAccurate2019} \cite{dunningComputingExtremelyAccurate2019} can have constant or weakly growing memory.
    }
    
    \citeauthor{maFrugalStreamingEstimating2014}\cite{maFrugalStreamingEstimating2014} introduce the randomized algorithms frugal streaming that requires even less memory.
    In Frugal-1U, the algorithm requires only one unit of memory to record the current quantile estimate. On arrival of each sample, the quantile estimate either stays unchanged or changed by a constant value. 
    % An improved version is Frugal-2U, which needs another memory unit for a better convergence rate. 
    
    To attack the same problem, \citeauthor{yazidiQuantileEstimationDynamic2016}\cite{yazidiQuantileEstimationDynamic2016} resort to the idea of online learning and achieves a similar method. 
    One distinction is step size. In this algorithm, each step the new estimate is updated by a variable related with the current estimate rather than a constant.
    Their work is inspired by \citeauthor{tierneySpaceEfficientRecursiveProcedure1983}\cite{tierneySpaceEfficientRecursiveProcedure1983}, who introduce stochastic learning method to quantile estimation problems. 
    % In The noticeable similarity and difference between \citeauthor{yazidiQuantileEstimationDynamic2016}'s work and the Frugal-1U is analysed in \textbf{section ??}.  

    In \textbf{section ?}, we provide a theoretical analysis and experiment comparisons on the noticeable similarity between the Frugal-1U and \citeauthor{yazidiQuantileEstimationDynamic2016}'s work . 

    
    \pagebreak 
    \section{Multi-quantile estimation}
    \label{multiquantile}
    When more quantile numbers are required, for example, two quantiles each identifying the upper and lower outlier of a distribution, the single quantile estimation methods become inefficient and less applicable.
    For only a few quantiles, this problem can be solved by running parallel single quantile estimation processes for each quantile. But the issue remains when the number of processes excesses the computer's parallel capacity.
    The algorithm which estimates several quantile values in one process, as a general solution to this problem, is then brought forward.
    Multiple quantile estimation is the simultaneous estimation on different quantile values from streaming data. 
    In general, it is the estimator that estimates $k$ quantiles (the $\tau_1$-, $\tau_2$-, $...$, $\tau_k$-quantiles) at the same time.
    Given the relationship between 
    It has been an issue targeted by different algorithms.\\\\

    \subsection{Online-histogram building method simultaneous estimation\cite{ben-haimStreamingParallelDecision}}
    The Streaming Parallel Decision Tree (SPDT) algorithm \cite{ben-haimStreamingParallelDecision} introduces an on-line histogram building method % from streaming data at parallel processors.
    in which histogram boundaries are estimated quantile values.
    In this method, multiple histograms are built from streaming data in parallel, which are then merged into a summary histogram of the entire dataset. The summary histogram is a set of sorted real numbers that represents the interval boundaries such that all the intervals have approximately the same size. Specifically, for a summary histogram with $N$ intervals, the set of real numbers is approximately the set of $\tau$-quantiles ($\tau = \frac{1}{N}, \frac{2}{N}, ..., \frac{N-1}{N}$) for the input data stream.

    This method works for distributed system where big data stream is processed by different processors. It also works well for huge amount of data because the computation complexity is not affected by the size of dataset.
        % This summary histogram is notable for its evenly distributed intervals sizes, as each interval has the same number of data points. To interpret the histogram into quantiles, 

        % excluded: \cite{pebayFormulasRobustOnepass2008}: online learning, not quantile\\\\


    \subsection{Other Method and Advantages of simultaneous estimation\cite{mcdermottDataSkeletonsSimultaneous2007}}

    Another single-pass low-memory methods for simultaneous multi-quantile estimation is the Data Skeleton(DS)\cite{mcdermottDataSkeletonsSimultaneous2007} algorithm, which is derived from the method proposed by \citeauthor{liechtySinglepassLowstorageArbitrary} \cite{liechtySinglepassLowstorageArbitrary}. For an estimation of $k$ quantiles, the algorithm requires the first $km$ data points($m$ is a constant) being sorted, and updates this tracking array on each new observation. Instead of the $k$ estimates of quantiles, it returns a total of $km$ estimates due to the redundancy of computation. This feature is considered an advantage for certain applications like density estimation, when extra quantile estimates is useful in accuracy improvement.


    Over the comparison with \citeauthor{liechtySinglepassLowstorageArbitrary}'s algorithm, \citeauthor{mcdermottDataSkeletonsSimultaneous2007}\cite{mcdermottDataSkeletonsSimultaneous2007} find simultaneous estimation on multiple quantiles has two main advantages over single quantile estimation methods. First is the save in computation time as it does not need to estimate quantiles separately. The second advantage, according to the experiments, is the accuracy improvement in simultaneous quantile estimation.  


    
    \textbf{
        Parallel quantile estimation using Stochastic approximation \cite{hammerSmoothEstimatesMultiple2019}
    }
    
    \textbf{P2 algorithm and latter algorithms that based on it}


% \section{Anomaly Detection and Outlier}

% \begin{enumerate}
%     \item Anomaly detection: \\
%         \cite{emmottMetaAnalysisAnomalyDetection2015}
%         (industrial use)
%         ()
%         % \cite{huangOnlineAnomalousTime2013}
% \end{enumerate}

\newpage
\bibliography{Thesis}
\bibliographystyle{IEEEtranN}

\end{document}
\end(documentclass)
@article{blassWhenAreTwo2008,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0811.0811},
  primaryClass = {cs},
  title = {When Are Two Algorithms the Same?},
  abstract = {People usually regard algorithms as more abstract than the programs that implement them. The natural way to formalize this idea is that algorithms are equivalence classes of programs with respect to a suitable equivalence relation. We argue that no such equivalence relation exists.},
  journal = {arXiv:0811.0811 [cs]},
  author = {Blass, Andreas and Dershowitz, Nachum and Gurevich, Yuri},
  month = nov,
  year = {2008},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - General Literature,Computer Science - Logic in Computer Science},
  file = {/Users/sue/Zotero/storage/3KVXJDD8/Blass et al. - 2008 - When are two algorithms the same.pdf;/Users/sue/Zotero/storage/J3S8QTM6/0811.html}
}

@article{emmottMetaAnalysisAnomalyDetection2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.01158},
  primaryClass = {cs, stat},
  title = {A {{Meta}}-{{Analysis}} of the {{Anomaly Detection Problem}}},
  abstract = {This article provides a thorough meta-analysis of the anomaly detection problem. To accomplish this we first identify approaches to benchmarking anomaly detection algorithms across the literature and produce a large corpus of anomaly detection benchmarks that vary in their construction across several dimensions we deem important to real-world applications: (a) point difficulty, (b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d) relevance of features. We apply a representative set of anomaly detection algorithms to this corpus, yielding a very large collection of experimental results. We analyze these results to understand many phenomena observed in previous work. First we observe the effects of experimental design on experimental results. Second, results are evaluated with two metrics, ROC Area Under the Curve and Average Precision. We employ statistical hypothesis testing to demonstrate the value (or lack thereof) of our benchmarks. We then offer several approaches to summarizing our experimental results, drawing several conclusions about the impact of our methodology as well as the strengths and weaknesses of some algorithms. Last, we compare results against a trivial solution as an alternate means of normalizing the reported performance of algorithms. The intended contributions of this article are many; in addition to providing a large publicly-available corpus of anomaly detection benchmarks, we provide an ontology for describing anomaly detection contexts, a methodology for controlling various aspects of benchmark creation, guidelines for future experimental design and a discussion of the many potential pitfalls of trying to measure success in this field.},
  language = {en},
  journal = {arXiv:1503.01158 [cs, stat]},
  author = {Emmott, Andrew and Das, Shubhomoy and Dietterich, Thomas and Fern, Alan and Wong, Weng-Keen},
  month = mar,
  year = {2015},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/sue/Zotero/storage/PF2M6YTA/Emmott et al. - 2015 - A Meta-Analysis of the Anomaly Detection Problem.pdf}
}

@article{ben-haimStreamingParallelDecision,
  title = {A {{Streaming Parallel Decision Tree Algorithm}}},
  abstract = {We propose a new algorithm for building decision tree classifiers. The algorithm is executed in a distributed environment and is especially designed for classifying large data sets and streaming data. It is empirically shown to be as accurate as a standard decision tree classifier, while being scalable for processing of streaming data on multiple processors. These findings are supported by a rigorous analysis of the algorithm's accuracy.},
  language = {en},
  author = {{Ben-Haim}, Yael and {Tom-Tov}, Elad},
  pages = {24},
  file = {/Users/sue/Zotero/storage/5QY79AUD/Ben-Haim and Tom-Tov - A Streaming Parallel Decision Tree Algorithm.pdf}
}

@article{hardtTrainFasterGeneralize2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.01240},
  primaryClass = {cs, math, stat},
  title = {Train Faster, Generalize Better: {{Stability}} of Stochastic Gradient Descent},
  shorttitle = {Train Faster, Generalize Better},
  abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions.},
  language = {en},
  journal = {arXiv:1509.01240 [cs, math, stat]},
  author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/sue/Zotero/storage/CSE3I8T2/Hardt et al. - 2015 - Train faster, generalize better Stability of stoc.pdf}
}

@article{jainP2AlgorithmDynamic1985,
  title = {The {{P2}} Algorithm for Dynamic Calculation of Quantiles and Histograms without Storing Observations},
  volume = {28},
  issn = {00010782},
  abstract = {A heuristic algorithm is proposed for dynamic calculation qf the median and other quantiles. The estimates are produced dynamically as the observations are generated. The observations are not stored; therefore, the algorithm has a very small and fixed storage requirement regardless of the number of observations. This makes it ideal for implementing in a quantile chip that can be used in industrial controllers and recorders. The algorithm is further extended to histogram plotting. The accuracy of the al,gorithm is analyzed.},
  language = {en},
  number = {10},
  journal = {Communications of the ACM},
  doi = {10.1145/4372.4378},
  author = {Jain, Raj and Chlamtac, Imrich},
  month = oct,
  year = {1985},
  pages = {1076-1085},
  file = {/Users/sue/Zotero/storage/US7XKXCT/Jain and Chlamtac - 1985 - The P2 algorithm for dynamic calculation of quanti.pdf}
}

@article{koenkerRegressionQuantiles1978,
  title = {Regression {{Quantiles}}},
  volume = {46},
  issn = {00129682},
  language = {en},
  number = {1},
  journal = {Econometrica},
  doi = {10.2307/1913643},
  author = {Koenker, Roger and Bassett, Gilbert},
  month = jan,
  year = {1978},
  pages = {33},
  file = {/Users/sue/Zotero/storage/WR4KA8JA/Koenker and Bassett - 1978 - Regression Quantiles.pdf}
}

@article{steinwartEstimatingConditionalQuantiles2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1102.2101},
  title = {Estimating Conditional Quantiles with the Help of the Pinball Loss},
  volume = {17},
  issn = {1350-7265},
  abstract = {The so-called pinball loss for estimating conditional quantiles is a well-known tool in both statistics and machine learning. So far, however, only little work has been done to quantify the efficiency of this tool for nonparametric approaches. We fill this gap by establishing inequalities that describe how close approximate pinball risk minimizers are to the corresponding conditional quantile. These inequalities, which hold under mild assumptions on the data-generating distribution, are then used to establish so-called variance bounds, which recently turned out to play an important role in the statistical analysis of (regularized) empirical risk minimization approaches. Finally, we use both types of inequalities to establish an oracle inequality for support vector machines that use the pinball loss. The resulting learning rates are min--max optimal under some standard regularity assumptions on the conditional quantile.},
  language = {en},
  number = {1},
  journal = {Bernoulli},
  doi = {10.3150/10-BEJ267},
  author = {Steinwart, Ingo and Christmann, Andreas},
  month = feb,
  year = {2011},
  keywords = {Mathematics - Statistics Theory},
  pages = {211-225},
  file = {/Users/sue/Zotero/storage/H5HYKNNY/Steinwart and Christmann - 2011 - Estimating conditional quantiles with the help of .pdf}
}

@article{maFrugalStreamingEstimating2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.1121},
  primaryClass = {cs},
  title = {Frugal {{Streaming}} for {{Estimating Quantiles}}:{{One}} (or Two) Memory Suffices},
  shorttitle = {Frugal {{Streaming}} for {{Estimating Quantiles}}},
  abstract = {Modern applications require processing streams of data for estimating statistical quantities such as quantiles with small amount of memory. In many such applications, in fact, one needs to compute such statistical quantities for each of a large number of groups, which additionally restricts the amount of memory available for the stream for any particular group. We address this challenge and introduce frugal streaming, that is algorithms that work with tiny \textendash{}typically, sub-streaming \textendash{} amount of memory per group.},
  language = {en},
  journal = {arXiv:1407.1121 [cs]},
  author = {Ma, Qiang and Muthukrishnan, S. and Sandler, Mark},
  month = jul,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Databases},
  file = {/Users/sue/Zotero/storage/BUA5VZNG/Ma et al. - 2014 - Frugal Streaming for Estimating QuantilesOne (or .pdf}
}

@techreport{pebayFormulasRobustOnepass2008,
  title = {Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments.},
  abstract = {We present a formula for the pairwise update of arbitrary-order centered statistical moments. This formula is of particular interest to compute such moments in parallel for large-scale, distributed data sets. As a corollary, we indicate a specialization of this formula for incremental updates, of particular interest to streaming implementations. Finally, we provide pairwise and incremental update formulas for the covariance.},
  language = {en},
  number = {SAND2008-6212, 1028931},
  doi = {10.2172/1028931},
  author = {Pebay, Philippe Pierre},
  month = sep,
  year = {2008},
  pages = {SAND2008-6212, 1028931},
  file = {/Users/sue/Zotero/storage/EKFQZ5WM/Pebay - 2008 - Formulas for robust, one-pass parallel computation.pdf}
}

@incollection{greenwaldQuantilesEquidepthHistograms2016,
  address = {{Berlin, Heidelberg}},
  title = {Quantiles and {{Equi}}-Depth {{Histograms}} over {{Streams}}},
  isbn = {978-3-540-28607-3 978-3-540-28608-0},
  language = {en},
  booktitle = {Data {{Stream Management}}},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-540-28608-0_3},
  author = {Greenwald, Michael B. and Khanna, Sanjeev},
  editor = {Garofalakis, Minos and Gehrke, Johannes and Rastogi, Rajeev},
  year = {2016},
  pages = {45-86},
  file = {/Users/sue/Zotero/storage/6XH46ADB/Greenwald and Khanna - 2016 - Quantiles and Equi-depth Histograms over Streams.pdf}
}

@incollection{huangOnlineAnomalousTime2013,
  address = {{Berlin, Heidelberg}},
  title = {An {{Online Anomalous Time Series Detection Algorithm}} for {{Univariate Data Streams}}},
  volume = {7906},
  isbn = {978-3-642-38576-6 978-3-642-38577-3},
  abstract = {We address the online anomalous time series detection problem among a set of series, combining three simple distance measures. This approach, akin to control charts, makes it easy to determine when a series begins to differ from other series. Empirical evidence shows that this novel online anomalous time series detection algorithm performs very well, while being efficient in terms of time complexity, when compared to approaches previously discussed in the literature.},
  language = {en},
  booktitle = {Recent {{Trends}} in {{Applied Artificial Intelligence}}},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-38577-3_16},
  author = {Huang, Huaming and Mehrotra, Kishan and Mohan, Chilukuri K.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ali, Moonis and Bosse, Tibor and Hindriks, Koen V. and Hoogendoorn, Mark and Jonker, Catholijn M. and Treur, Jan},
  year = {2013},
  pages = {151-160},
  file = {/Users/sue/Zotero/storage/YGGN44NJ/Huang et al. - 2013 - An Online Anomalous Time Series Detection Algorith.pdf}
}

@article{zhengGradientDescentAlgorithms2011,
  title = {Gradient Descent Algorithms for Quantile Regression with Smooth Approximation},
  volume = {2},
  issn = {1868-8071, 1868-808X},
  abstract = {Gradient based optimization methods often converge quickly to a local optimum. However, the check loss function used by quantile regression model is not everywhere differentiable, which prevents the gradient based optimization methods from being applicable. As such, this paper introduces a smooth function to approximate the check loss function so that the gradient based optimization methods could be employed for fitting quantile regression model. The properties of the smooth approximation are discussed. Two algorithms are proposed for minimizing the smoothed objective function. The first method directly applies gradient descent, resulting the gradient descent smooth quantile regression model; the second approach minimizes the smoothed objective function in the framework of functional gradient descent by changing the fitted model along the negative gradient direction in each iteration, which yields boosted smooth quantile regression algorithm. Extensive experiments on simulated data and real-world data show that, compared to alternative quantile regression models, the proposed smooth quantile regression algorithms can achieve higher prediction accuracy and are more efficient in removing noninformative predictors.},
  language = {en},
  number = {3},
  journal = {International Journal of Machine Learning and Cybernetics},
  doi = {10.1007/s13042-011-0031-2},
  author = {Zheng, Songfeng},
  month = sep,
  year = {2011},
  pages = {191-207},
  file = {/Users/sue/Zotero/storage/4ULXALN2/Zheng - 2011 - Gradient descent algorithms for quantile regressio.pdf}
}

@article{freundDecisionTheoreticGeneralizationOnLine1997,
  title = {A {{Decision}}-{{Theoretic Generalization}} of {{On}}-{{Line Learning}} and an {{Application}} to {{Boosting}}},
  volume = {55},
  issn = {0022-0000},
  abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone\textendash{}Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  doi = {10.1006/jcss.1997.1504},
  author = {Freund, Yoav and Schapire, Robert E},
  month = aug,
  year = {1997},
  pages = {119-139},
  file = {/Users/sue/Zotero/storage/8WS72FHI/Freund and Schapire - 1997 - A Decision-Theoretic Generalization of On-Line Lea.pdf;/Users/sue/Zotero/storage/CBPVDZDY/S002200009791504X.html}
}

@article{shamirStochasticGradientDescent,
  title = {Stochastic {{Gradient Descent}} for {{Non}}-Smooth {{Optimization}}: {{Convergence Results}} and {{Optimal Averaging Schemes}}},
  abstract = {Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required nontrivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimalit{$\surd$}y of the last SGD iterate scales as O(log(T )/ T ) for non-smooth convex objective functions, and O(log(T )/T ) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some experimental illustrations.},
  language = {en},
  author = {Shamir, Ohad and Zhang, Tong},
  pages = {9},
  file = {/Users/sue/Zotero/storage/VEPFGNVH/Shamir and Zhang - Stochastic Gradient Descent for Non-smooth Optimiz.pdf}
}

@article{yangRSGBeatingSubgradient,
  title = {{{RSG}}: {{Beating Subgradient Method}} without {{Smoothness}} and {{Strong Convexity}}},
  language = {en},
  author = {Yang, Tianbao and Lin, Qihang},
  pages = {33},
  file = {/Users/sue/Zotero/storage/TZSIQIET/Yang and Lin - RSG Beating Subgradient Method without Smoothness.pdf}
}

@article{ouyangStochasticSmoothingNonsmooth2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1205.4481},
  primaryClass = {cs, stat},
  title = {Stochastic {{Smoothing}} for {{Nonsmooth Minimizations}}: {{Accelerating SGD}} by {{Exploiting Structure}}},
  shorttitle = {Stochastic {{Smoothing}} for {{Nonsmooth Minimizations}}},
  abstract = {In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.},
  journal = {arXiv:1205.4481 [cs, stat]},
  author = {Ouyang, Hua and Gray, Alexander},
  month = may,
  year = {2012},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Computation},
  file = {/Users/sue/Zotero/storage/WMC8HEY3/Ouyang and Gray - 2012 - Stochastic Smoothing for Nonsmooth Minimizations .pdf;/Users/sue/Zotero/storage/D35G6A3Y/1205.html}
}

@inproceedings{yiuEfficientQuantileRetrieval2006,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Efficient {{Quantile Retrieval}} on {{Multi}}-Dimensional {{Data}}},
  isbn = {978-3-540-32961-9},
  abstract = {Given a set of N multi-dimensional points, we study the computation of {$\varphi$}-quantiles according to a ranking function F, which is provided by the user at runtime. Specifically, F computes a score based on the coordinates of each point; our objective is to report the object whose score is the {$\varphi$}N-th smallest in the dataset. {$\varphi$}-quantiles provide a succinct summary about the F-distribution of the underlying data, which is useful for online decision support, data mining, selectivity estimation, query optimization, etc. Assuming that the dataset is indexed by a spatial access method, we propose several algorithms for retrieving a quantile efficiently. Analytical and experimental results demonstrate that a branch-and-bound method is highly effective in practice, outperforming alternative approaches by a significant factor.},
  language = {en},
  booktitle = {Advances in {{Database Technology}} - {{EDBT}} 2006},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Yiu, Man Lung and Mamoulis, Nikos and Tao, Yufei},
  editor = {Ioannidis, Yannis and Scholl, Marc H. and Schmidt, Joachim W. and Matthes, Florian and Hatzopoulos, Mike and Boehm, Klemens and Kemper, Alfons and Grust, Torsten and Boehm, Christian},
  year = {2006},
  keywords = {Multidimensional Data,Query Point,Range Count,Ranking Function,Skyline Query},
  pages = {167-185},
  file = {/Users/sue/Zotero/storage/BWIIFGX2/Yiu et al. - 2006 - Efficient Quantile Retrieval on Multi-dimensional .pdf}
}

@article{felberRandomizedOnlineQuantile2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.01156},
  primaryClass = {cs},
  title = {A Randomized Online Quantile Summary in \${{O}}(\textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\} \textbackslash{}log \textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\})\$ Words},
  abstract = {A quantile summary is a data structure that approximates to \$\textbackslash{}varepsilon\$-relative error the order statistics of a much larger underlying dataset. In this paper we develop a randomized online quantile summary for the cash register data input model and comparison data domain model that uses \$O(\textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\} \textbackslash{}log \textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\})\$ words of memory. This improves upon the previous best upper bound of \$O(\textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\} \textbackslash{}log\^\{3/2\} \textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\})\$ by Agarwal et. al. (PODS 2012). Further, by a lower bound of Hung and Ting (FAW 2010) no deterministic summary for the comparison model can outperform our randomized summary in terms of space complexity. Lastly, our summary has the nice property that \$O(\textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\} \textbackslash{}log \textbackslash{}frac\{1\}\{\textbackslash{}varepsilon\})\$ words suffice to ensure that the success probability is \$1 - e\^\{-\textbackslash{}text\{poly\}(1/\textbackslash{}varepsilon)\}\$.},
  journal = {arXiv:1503.01156 [cs]},
  author = {Felber, David and Ostrovsky, Rafail},
  month = mar,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/sue/Zotero/storage/KWQTKF88/Felber and Ostrovsky - 2015 - A randomized online quantile summary in $O(frac 1.pdf;/Users/sue/Zotero/storage/EN3DC5XG/1503.html}
}

@article{guhaStreamOrderOrder2009,
  title = {Stream {{Order}} and {{Order Statistics}}: {{Quantile Estimation}} in {{Random}}-{{Order Streams}}},
  volume = {38},
  issn = {0097-5397},
  shorttitle = {Stream {{Order}} and {{Order Statistics}}},
  abstract = {When trying to process a data stream in small space, how important is the order in which the data arrive? Are there problems that are unsolvable when the ordering is worst case, but that can be solved (with high probability) when the order is chosen uniformly at random? If we consider the stream as if ordered by an adversary, what happens if we restrict the power of the adversary? We study these questions in the context of quantile estimation, one of the most well studied problems in the data-stream model. Our results include an \$O(\$polylog \$n)\$-space, \$O(\textbackslash{}log\textbackslash{}log n)\$-pass algorithm for exact selection in a randomly ordered stream of n elements. This resolves an open question of Munro and Paterson [Theoret. Comput. Sci., 23 (1980), pp. 315\textendash{}323]. We then demonstrate an exponential separation between the random-order and adversarial-order models: using \$O(\$polylog \$n)\$ space, exact selection requires \$\textbackslash{}Omega(\textbackslash{}log n/\textbackslash{}log\textbackslash{}log n)\$ passes in the adversarial-order model. This lower bound, in contrast to previous results, applies to fully general randomized algorithms and is established via a new bound on the communication complexity of a natural pointer-chasing style problem. We also prove the first fully general lower bounds in the random-order model: finding an element with rank \$n/2\textbackslash{}pm n\^\{\textbackslash{}delta\}\$ in the single-pass random-order model with probability at least \$9/10\$ requires \$\textbackslash{}Omega(\textbackslash{}sqrt\{n\^\{1-3\textbackslash{}delta\}/\textbackslash{}log n\})\$ space.},
  number = {5},
  journal = {SIAM Journal on Computing},
  doi = {10.1137/07069328X},
  author = {Guha, S. and McGregor, A.},
  month = jan,
  year = {2009},
  pages = {2044-2059},
  file = {/Users/sue/Zotero/storage/W4A8HBP4/Guha and McGregor - 2009 - Stream Order and Order Statistics Quantile Estima.pdf;/Users/sue/Zotero/storage/XRTFKI4K/07069328X.html}
}

@incollection{dobraAMSSketch2009,
  address = {{Boston, MA}},
  title = {{{AMS Sketch}}},
  isbn = {978-0-387-39940-9},
  language = {en},
  booktitle = {Encyclopedia of {{Database Systems}}},
  publisher = {{Springer US}},
  doi = {10.1007/978-0-387-39940-9_16},
  author = {Dobra, Alin},
  editor = {LIU, LING and {\"O}ZSU, M. TAMER},
  year = {2009},
  pages = {80-83},
  file = {/Users/sue/Zotero/storage/JL2HYAE5/Dobra - 2009 - AMS Sketch.pdf}
}

@article{sangnierJointQuantileRegression,
  title = {Joint Quantile Regression in Vector-Valued {{RKHSs}}},
  abstract = {Addressing the will to give a more complete picture than an average relationship provided by standard regression, a novel framework for estimating and predicting simultaneously several conditional quantiles is introduced. The proposed methodology leverages kernel-based multi-task learning to curb the embarrassing phenomenon of quantile crossing, with a one-step estimation procedure and no post-processing. Moreover, this framework comes along with theoretical guarantees and an efficient coordinate descent learning algorithm. Numerical experiments on benchmark and real datasets highlight the enhancements of our approach regarding the prediction error, the crossing occurrences and the training time.},
  language = {en},
  author = {Sangnier, Maxime and Fercoq, Olivier},
  pages = {19},
  file = {/Users/sue/Zotero/storage/SITM7K4L/Sangnier and Fercoq - Joint quantile regression in vector-valued RKHSs.pdf}
}

@article{hongEstimatingQuantileSensitivities2009,
  title = {Estimating {{Quantile Sensitivities}}},
  volume = {57},
  issn = {0030-364X, 1526-5463},
  language = {en},
  number = {1},
  journal = {Operations Research},
  doi = {10.1287/opre.1080.0531},
  author = {Hong, L. Jeff},
  month = feb,
  year = {2009},
  pages = {118-130},
  file = {/Users/sue/Zotero/storage/8FLHM955/Hong - 2009 - Estimating Quantile Sensitivities.pdf}
}

@article{rayArtApproximatingDistributions1800,
  title = {The {{Art}} of {{Approximating Distributions}}: {{Histograms}} and {{Quantiles}} at {{Scale}}},
  language = {en},
  author = {Ray, Nelson},
  year = {1800},
  pages = {8},
  file = {/Users/sue/ANU_study/2019_sem2/Honors/Readings/The Art of Approximating Distributions_ Histograms and Quantiles at Scale.pdf}
}



{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from Distro_generation import get_dataset, get_q_batches, get_q_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# c_Norm = colors.Normalize(vmin=0, vmax=1)\n",
    "# scalarMap = cmx.ScalarMappable(norm=c_Norm, cmap=plt.get_cmap('cool'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_fd = 'Experiment_results/'\n",
    "fd_lst = ['Frugal_SGD', 'SGD']\n",
    "for fd in fd_lst:\n",
    "    if not os.path.exists(main_fd+fd):\n",
    "        os.makedirs(main_fd+fd)\n",
    "        \n",
    "sgd_lst = ['distro', 'data_size', 'step_size', 'data_sequence']        \n",
    "for fd in sgd_lst:\n",
    "    fd_name = main_fd+'SGD/'+fd\n",
    "    if not os.path.exists(fd_name):\n",
    "        os.makedirs(fd_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_settings(distro_lst, datasize_lst, stepsize_lst):\n",
    "    len_lst = [len(distro_lst), len(datasize_lst), len(stepsize_lst)]\n",
    "    if len_lst.count(1) != len(len_lst)-1: raise Exception(\"Setting inputs are wrong!\")\n",
    "    \n",
    "    N_settings = max((len_lst))\n",
    "    setting_lst = []\n",
    "    for lst in [distro_lst, datasize_lst, stepsize_lst]:\n",
    "        if len(lst)==1: \n",
    "            lst = lst*N_settings\n",
    "        setting_lst.append(lst)\n",
    "        \n",
    "    changed_setting = None\n",
    "    change_lst = ['distro', 'data_size', 'step_size']\n",
    "    for idx, l in enumerate(len_lst):\n",
    "        if l>1: changed_setting = change_lst[idx]\n",
    "    return np.asarray(setting_lst).T, changed_setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_stepsize(k, stepsize, length):\n",
    "    if stepsize=='const':\n",
    "        return 1 * np.ones(length)\n",
    "    elif stepsize=='2_div_sqrt_k':\n",
    "        return 2/math.sqrt(k)* np.ones(length)\n",
    "    elif stepsize=='0.002_div_sqrt_k':\n",
    "        return 0.002/math.sqrt(k) * np.ones(length)\n",
    "    raise Exception('stepsize parameter is wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_procs(dataset, step_size, tau_lst, update_size = 200):\n",
    "    if update_size*5 > dataset.shape[0]:\n",
    "        print (\"Warning!\",\n",
    "            \"Cannot do the step size trick because the dataset of size {} is too small for the update size {}\"\n",
    "            .format(dataset.shape[0], update_size))\n",
    "    proc = np.zeros((dataset.shape[0], len(tau_lst)))\n",
    "    q_pre, q_arr = np.zeros(len(tau_lst)), np.zeros(len(tau_lst))\n",
    "    alpha_adaptation = np.ones(len(tau_lst))\n",
    "\n",
    "    for k, x in enumerate(dataset):          \n",
    "        if k % update_size == 0 and k >0:\n",
    "#             print (k,q_arr-q_pre)\n",
    "            update_var = update_stepsize(alpha_arr, q_arr-q_pre, step_size, update_size)\n",
    "#             print ('update_var', update_var)\n",
    "            alpha_adaptation = alpha_adaptation * update_var\n",
    "#             print ('alpha_adaptation', alpha_adaptation)\n",
    "            q_pre = [i for i in q_arr]\n",
    "        if step_size != 'frugal': \n",
    "            alpha_arr = set_stepsize(k+1, step_size, len(tau_lst))* alpha_adaptation\n",
    "#             if k % update_size == 0: print('alpha_arr', alpha_arr)\n",
    "        for i, q in enumerate(q_arr):\n",
    "            tau = tau_lst[i]\n",
    "            alpha = alpha_arr[i]\n",
    "            if step_size != 'frugal':\n",
    "                q = sgd(q, x, tau, alpha)\n",
    "            else: \n",
    "                q = frugal(q, x, tau)\n",
    "            q_arr[i] = q\n",
    "        proc[k] = q_arr\n",
    "#     print (alpha_arr)\n",
    "    return proc.T\n",
    "\n",
    "def update_stepsize(alpha_arr, diff, step_size, update_size):\n",
    "    update_var = np.ones(len(diff))\n",
    "    for i, a in enumerate(alpha_arr):\n",
    "        d = diff[i]\n",
    "        if a * update_size * 0.25 < abs(d): update_var[i] = 2\n",
    "        elif a * update_size * 0.02 > abs(d): update_var[i] = 1/2\n",
    "#     print ('step_size', update_var)\n",
    "    return update_var\n",
    "    \n",
    "def sgd(q, x, tau, alpha):\n",
    "    if x > q:\n",
    "        q = q + alpha*tau\n",
    "    else:\n",
    "        q = q - alpha*(1-tau)  \n",
    "    return q\n",
    "    \n",
    "def frugal(q, x, tau):\n",
    "    rdn = np.random.uniform()\n",
    "    if x > q and rdn > 1-tau:\n",
    "        q += 1\n",
    "    elif x < q and rdn > tau:\n",
    "        q -= 1\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res(procs):\n",
    "    if len(procs.shape)!=2:raise Exception('Procs of wrong shape:' + str(procs.shape)+ ', should be 2d array')\n",
    "    return procs[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_ests(dataset, step_size, tau_lst):\n",
    "\n",
    "    if len(dataset.shape)>2:\n",
    "        raise Exception('Dataset for q_est calculation of wrong shape:' + str(dataset.shape)+ ', should be 1d or 2d array')\n",
    "    if len(dataset.shape)==1:\n",
    "        procs = get_procs(dataset, step_size, tau_lst)\n",
    "        res = get_res(procs)\n",
    "    else:\n",
    "        res = np.zeros((dataset.shape[0], len(tau_lst)))\n",
    "        procs = np.zeros((dataset.shape[0], len(tau_lst), dataset.shape[1]))\n",
    "        for idx, dt in enumerate(dataset):\n",
    "            procs[idx] = get_procs(dt, step_size, tau_lst)\n",
    "            res[idx] = get_res(procs[idx])\n",
    "    return res, procs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_e(true, batches, est):\n",
    "    upper = est - batches\n",
    "    bottom = true - batches\n",
    "    return (upper/bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(data, filename, setting=None):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        if setting is not None: outfile.write('# Setting: {0}\\n'.format(setting))\n",
    "        if len(data.shape) == 1: data = data.reshape(-1, 1)\n",
    "        outfile.write('# Array shape: {0}\\n \\n'.format(data.shape))\n",
    "\n",
    "        for data_slice in data:\n",
    "            np.savetxt(outfile, data_slice, fmt='%-15.8g')\n",
    "            outfile.write('\\n')\n",
    "\n",
    "\n",
    "def write_data_overview(category, setting, tau_lst, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(\"Tested on \"+category+\": \"+str(setting)+\"\\n\") \n",
    "        f.write(str(tau_lst))\n",
    "    \n",
    "# write_data_overview('ca', 'se', tau_vals, \"try.txt\")\n",
    "    \n",
    "def save_data(foldername, file_name, tau_lst, data_dict):\n",
    "    print (\"save data in folder\", foldername)\n",
    "    pathlib.Path(foldername).mkdir(parents=True, exist_ok=True) \n",
    "    category, setting = file_name[0], file_name[1]    \n",
    "    write_data_overview(category, setting, tau_lst, foldername+str(setting)+\"_\"+\"overview.txt\")\n",
    "\n",
    "    for data_name in data_dict:\n",
    "        print (foldername+str(setting)+\"_\"+data_name+'.txt')\n",
    "        write_data(data_dict[data_name], foldername+str(setting)+\"_\"+data_name+'.txt')\n",
    "        \n",
    "    return\n",
    "\n",
    "# save_data(np.random.uniform(0, 1, 100), 'Experiment_results/Adaptive_stepsize/distro/mix_overview.txt')\n",
    "# save_data('', ('C', 'S'), tau_vals, {'dataset': np.reshape(np.random.uniform(0, 4, 100), (2, 5, 10))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(changed_setting, distro, datasize, stepsize, s_test):\n",
    "#     print(changed_setting)\n",
    "#     if s_test:\n",
    "#         return ('Shuffle', True)\n",
    "    if changed_setting=='distro':\n",
    "        return ('Distribution', distro)\n",
    "    elif changed_setting=='data_size':\n",
    "        return ('Data size', datasize)\n",
    "    elif changed_setting=='step_size':\n",
    "        return ('Step size', stepsize)\n",
    "    else: raise Exception ('Cannot get file name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffled_dataset(dataset, datasize, s_test):\n",
    "    if not s_test: return dataset\n",
    "    \n",
    "    shuffled_dt = np.zeros((N_s, datasize))\n",
    "    for i in range(N_s):\n",
    "        np.random.shuffle(dataset)\n",
    "        shuffled_dt[i] = dataset\n",
    "    dataset = shuffled_dt\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "distros = ['mix', 'gau_1', 'gau_2', 'exp']\n",
    "stepsizes = ['const', '2_div_sqrt_k', '0.002_div_sqrt_k']\n",
    "tau_vals = [0.1, 0.3, 0.5, 0.9, 0.99]\n",
    "N_g = 12 # N_generation\n",
    "N_s = 10 # N_shuffle\n",
    "\n",
    "N_q = len(tau_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_sgd_compare(folder_name, distro_lst, datasize_lst, stepsize_lst, \n",
    "                         g_test=False, s_test=False, tau_lst=tau_vals, \n",
    "                        ):\n",
    "    \n",
    "    if g_test and s_test: raise Exception(\"g_test and s_test can't both be true\")\n",
    "    \n",
    "    # generate different settings\n",
    "    setting_lst, changed_setting = get_settings(distro_lst, datasize_lst, stepsize_lst,)\n",
    "    print (setting_lst)\n",
    "    \n",
    "    # if only stepsize changes, generate dataset and q_batches\n",
    "    dataset, q_batches = 0, 0\n",
    "    if len(distro_lst)==1 and len(datasize_lst)==1:\n",
    "        dataset = get_dataset(distro_lst[0], datasize_lst[0], g_test)\n",
    "        q_batches = get_q_batches(dataset, tau_lst)\n",
    "        \n",
    "    # for each setting = [distro, datasize, stepsize]\n",
    "    for idx, setting in enumerate(setting_lst):\n",
    "        \n",
    "        # generate all the data\n",
    "        distro, datasize, stepsize = setting[0], int(setting[1]), setting[2]\n",
    "        q_true = get_q_true(distro, tau_lst)\n",
    "        if len(distro_lst)!=1 or len(datasize_lst)!=1:\n",
    "            dataset = get_dataset(distro, datasize, g_test)\n",
    "            q_batches = get_q_batches(dataset, tau_lst)\n",
    "        if not s_test: \n",
    "            dataset = get_shuffled_dataset(dataset, datasize, s_test)\n",
    "        q_est_res, q_est_proc = get_q_ests(dataset, stepsize, tau_lst)\n",
    "        E = get_normalized_e(q_true, q_batches, q_est_res)\n",
    "        \n",
    "        data_dict = {\n",
    "            'q_true': q_true,\n",
    "            'q_batches': q_batches,\n",
    "            'q_est_res': q_est_res,\n",
    "            'q_est_proc': q_est_proc,\n",
    "            'E': E\n",
    "        }\n",
    "#         print(E)\n",
    "#         # generate charts and tables?\n",
    "        file_name = get_file_name(changed_setting, distro, datasize, stepsize, s_test)\n",
    "        save_data(folder_name, file_name, tau_lst, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_lst = ['data_size',]        \n",
    "\n",
    "def all_sgd_comparisons(sgd_lst, root_folder='Adaptive_stepsize'):\n",
    "    for t in sgd_lst:\n",
    "        print (t)\n",
    "        folder_name = \"Experiment_results/{}/{}/\".format(root_folder, t)\n",
    "        if t=='distro':\n",
    "            quantile_sgd_compare(folder_name, distros, [1000], ['const'], True)\n",
    "        elif t=='data_size':\n",
    "            quantile_sgd_compare(folder_name, ['gau_2'], [100, 1000, 10000], ['const'], True)\n",
    "        elif t=='step_size':\n",
    "            quantile_sgd_compare(folder_name, ['gau_1'], [1000], stepsizes, True)\n",
    "        elif t=='data_sequence':\n",
    "            quantile_sgd_compare(folder_name, ['gau_1', 'gau_1'], [1000], ['const'], False, True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_size\n",
      "[['gau_2' '100' 'const']\n",
      " ['gau_2' '1000' 'const']\n",
      " ['gau_2' '10000' 'const']]\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "Warning! Cannot do the step size trick because the dataset of size 100 is too small for the update size 200\n",
      "save data in folder Experiment_results/Adaptive_stepsize/data_size/\n",
      "Experiment_results/Adaptive_stepsize/data_size/100_q_true.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/100_q_batches.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/100_q_est_res.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/100_q_est_proc.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/100_E.txt\n",
      "save data in folder Experiment_results/Adaptive_stepsize/data_size/\n",
      "Experiment_results/Adaptive_stepsize/data_size/1000_q_true.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/1000_q_batches.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/1000_q_est_res.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/1000_q_est_proc.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/1000_E.txt\n",
      "save data in folder Experiment_results/Adaptive_stepsize/data_size/\n",
      "Experiment_results/Adaptive_stepsize/data_size/10000_q_true.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/10000_q_batches.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/10000_q_est_res.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/10000_q_est_proc.txt\n",
      "Experiment_results/Adaptive_stepsize/data_size/10000_E.txt\n"
     ]
    }
   ],
   "source": [
    "# Run all functions\n",
    "main_folder = 'Adaptive_stepsize'\n",
    "all_sgd_comparisons(adaptive_lst, main_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sgd_frugal_compare(distro_lst, datasize, tau_lst=tau_vals):\n",
    "#     #distro changes, use the biggest datasize, do not shuffle\n",
    "#     for distro in distro_lst:\n",
    "#         q_true = get_q_true(distro, tau_lst)\n",
    "#         dataset = get_dataset(distro, datasize, False)\n",
    "#         q_batches = get_q_batches(dataset, tau_lst)\n",
    "        \n",
    "#         sgd_res, sgd_proc = get_q_ests(dataset, 'const', tau_lst)\n",
    "        \n",
    "#         N_frugal = 20\n",
    "#         frugal_res = np.zeros((N_frugal, len(tau_lst)))\n",
    "#         frugal_proc = np.zeros((N_frugal, len(tau_lst), datasize))\n",
    "#         for i in range(N_frugal):\n",
    "#             frugal_res[i], frugal_proc[i] = get_q_ests(dataset, 'frugal', tau_lst)\n",
    "        \n",
    "#         ax_name = 'Tested on '+distro+' distritbution with '+str(datasize)+' data points'\n",
    "#         fig, lgd = plot_procs(ax_name, tau_vals, q_true, frugal_proc, sgd_proc)\n",
    "#         title = fig.suptitle('Quantile Estimation: Frugal vs SGD')\n",
    "\n",
    "#         fd = \"Experiment_results/Frugal_SGD/\"\n",
    "#         plt.savefig(fd+distro+'.png', bbox_extra_artists=(lgd, title), bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "# sgd_frugal_compare(distros, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "### Always have $q_k = x$ for each x in the data stream\n",
    "\n",
    "When $x - q_k > 0$, we have $l(q_k) = \\tau(x-q_k)$:\n",
    "\\begin{align}\n",
    "q_{k+1} & = q_k - \\frac{l(q_k)}{l'(q_k)} \\\\\n",
    "        & = q_k - \\frac{\\tau(x-q_k)}{-\\tau} \\\\\n",
    "        & = q_k - (- x + q_k) \\\\\n",
    "        & = x\n",
    "\\end{align}\n",
    "\n",
    "Same happens when $x - q_k < 0$\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_results/Adaptive_stepsize/distro/gau_1_\n",
      "Experiment_results/Adaptive_stepsize/distro/mix_\n",
      "Experiment_results/Adaptive_stepsize/distro/gau_2_\n",
      "Experiment_results/Adaptive_stepsize/distro/exp_\n",
      "Experiment_results/Adaptive_stepsize/data_size/100_\n",
      "Experiment_results/Adaptive_stepsize/data_size/10000_\n",
      "Experiment_results/Adaptive_stepsize/data_size/1000_\n"
     ]
    }
   ],
   "source": [
    "from Plot import plot_charts\n",
    "plot_charts(\"Experiment_results/{}/\".format(main_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
